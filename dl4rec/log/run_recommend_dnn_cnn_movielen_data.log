E:\Anaconda3_4.2.0_64bit\python.exe D:/code/movie_recommender/recommend_dnn_cnn_movielen_data.py
2018-04-07 21:41:57.754496: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2018-04-07 21:41:57.755527: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-07 21:41:57.756386: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-07 21:41:57.757602: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-07 21:41:57.758143: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-07 21:41:57.759199: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-04-07 21:41:57.759794: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-04-07 21:41:57.760634: W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-04-07 21:42:00.012999: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:887] Found device 0 with properties:
name: GeForce GTX 950M
major: 5 minor: 0 memoryClockRate (GHz) 1.124
pciBusID 0000:01:00.0
Total memory: 4.00GiB
Free memory: 3.35GiB
2018-04-07 21:42:00.013874: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:908] DMA: 0
2018-04-07 21:42:00.014322: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:918] 0:   Y
2018-04-07 21:42:00.040165: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0)
Writing to D:\code\movie_recommender\runs\1523108521

2018-04-07T21:42:34.163771: Epoch   0 Batch    0/3125   train_loss = 22.745
2018-04-07T21:42:35.891418: Epoch   0 Batch   20/3125   train_loss = 5.530
2018-04-07T21:42:37.574363: Epoch   0 Batch   40/3125   train_loss = 4.324
2018-04-07T21:42:39.266144: Epoch   0 Batch   60/3125   train_loss = 2.925
2018-04-07T21:42:41.063440: Epoch   0 Batch   80/3125   train_loss = 2.373
2018-04-07T21:42:42.724506: Epoch   0 Batch  100/3125   train_loss = 2.148
2018-04-07T21:42:44.391633: Epoch   0 Batch  120/3125   train_loss = 2.001
2018-04-07T21:42:46.073716: Epoch   0 Batch  140/3125   train_loss = 2.263
2018-04-07T21:42:47.767273: Epoch   0 Batch  160/3125   train_loss = 1.792
2018-04-07T21:42:49.431708: Epoch   0 Batch  180/3125   train_loss = 1.812
2018-04-07T21:42:51.116678: Epoch   0 Batch  200/3125   train_loss = 1.735
2018-04-07T21:42:52.778706: Epoch   0 Batch  220/3125   train_loss = 1.636
2018-04-07T21:42:54.444673: Epoch   0 Batch  240/3125   train_loss = 1.472
2018-04-07T21:42:56.094519: Epoch   0 Batch  260/3125   train_loss = 1.559
2018-04-07T21:42:57.780691: Epoch   0 Batch  280/3125   train_loss = 1.794
2018-04-07T21:42:59.457469: Epoch   0 Batch  300/3125   train_loss = 1.710
2018-04-07T21:43:01.124376: Epoch   0 Batch  320/3125   train_loss = 1.629
2018-04-07T21:43:02.782280: Epoch   0 Batch  340/3125   train_loss = 1.466
2018-04-07T21:43:04.455908: Epoch   0 Batch  360/3125   train_loss = 1.443
2018-04-07T21:43:06.126031: Epoch   0 Batch  380/3125   train_loss = 1.532
2018-04-07T21:43:07.821981: Epoch   0 Batch  400/3125   train_loss = 1.280
2018-04-07T21:43:09.508522: Epoch   0 Batch  420/3125   train_loss = 1.306
2018-04-07T21:43:11.175957: Epoch   0 Batch  440/3125   train_loss = 1.443
2018-04-07T21:43:12.847399: Epoch   0 Batch  460/3125   train_loss = 1.382
2018-04-07T21:43:14.622668: Epoch   0 Batch  480/3125   train_loss = 1.454
2018-04-07T21:43:16.324190: Epoch   0 Batch  500/3125   train_loss = 1.073
2018-04-07T21:43:17.989673: Epoch   0 Batch  520/3125   train_loss = 1.382
2018-04-07T21:43:19.652189: Epoch   0 Batch  540/3125   train_loss = 1.359
2018-04-07T21:43:21.318617: Epoch   0 Batch  560/3125   train_loss = 1.437
2018-04-07T21:43:22.977658: Epoch   0 Batch  580/3125   train_loss = 1.526
2018-04-07T21:43:24.649036: Epoch   0 Batch  600/3125   train_loss = 1.400
2018-04-07T21:43:26.333680: Epoch   0 Batch  620/3125   train_loss = 1.433
2018-04-07T21:43:28.004224: Epoch   0 Batch  640/3125   train_loss = 1.366
2018-04-07T21:43:29.676279: Epoch   0 Batch  660/3125   train_loss = 1.412
2018-04-07T21:43:31.348772: Epoch   0 Batch  680/3125   train_loss = 1.311
2018-04-07T21:43:33.055857: Epoch   0 Batch  700/3125   train_loss = 1.400
2018-04-07T21:43:34.744387: Epoch   0 Batch  720/3125   train_loss = 1.257
2018-04-07T21:43:36.417527: Epoch   0 Batch  740/3125   train_loss = 1.390
2018-04-07T21:43:38.192794: Epoch   0 Batch  760/3125   train_loss = 1.306
2018-04-07T21:43:39.862901: Epoch   0 Batch  780/3125   train_loss = 1.391
2018-04-07T21:43:41.526289: Epoch   0 Batch  800/3125   train_loss = 1.348
2018-04-07T21:43:43.204783: Epoch   0 Batch  820/3125   train_loss = 1.281
2018-04-07T21:43:44.882763: Epoch   0 Batch  840/3125   train_loss = 1.259
2018-04-07T21:43:46.561806: Epoch   0 Batch  860/3125   train_loss = 1.264
2018-04-07T21:43:48.242814: Epoch   0 Batch  880/3125   train_loss = 1.291
2018-04-07T21:43:49.908282: Epoch   0 Batch  900/3125   train_loss = 1.238
2018-04-07T21:43:51.572805: Epoch   0 Batch  920/3125   train_loss = 1.276
2018-04-07T21:43:53.229705: Epoch   0 Batch  940/3125   train_loss = 1.349
2018-04-07T21:43:54.879139: Epoch   0 Batch  960/3125   train_loss = 1.281
2018-04-07T21:43:56.584759: Epoch   0 Batch  980/3125   train_loss = 1.344
2018-04-07T21:43:58.265234: Epoch   0 Batch 1000/3125   train_loss = 1.234
2018-04-07T21:43:59.934244: Epoch   0 Batch 1020/3125   train_loss = 1.420
2018-04-07T21:44:01.606188: Epoch   0 Batch 1040/3125   train_loss = 1.211
2018-04-07T21:44:03.278684: Epoch   0 Batch 1060/3125   train_loss = 1.494
2018-04-07T21:44:05.034955: Epoch   0 Batch 1080/3125   train_loss = 1.218
2018-04-07T21:44:06.692896: Epoch   0 Batch 1100/3125   train_loss = 1.387
2018-04-07T21:44:08.355865: Epoch   0 Batch 1120/3125   train_loss = 1.310
2018-04-07T21:44:10.015241: Epoch   0 Batch 1140/3125   train_loss = 1.292
2018-04-07T21:44:11.689987: Epoch   0 Batch 1160/3125   train_loss = 1.285
2018-04-07T21:44:13.463699: Epoch   0 Batch 1180/3125   train_loss = 1.249
2018-04-07T21:44:15.127992: Epoch   0 Batch 1200/3125   train_loss = 1.265
2018-04-07T21:44:16.799960: Epoch   0 Batch 1220/3125   train_loss = 1.209
2018-04-07T21:44:18.472405: Epoch   0 Batch 1240/3125   train_loss = 1.176
2018-04-07T21:44:20.158989: Epoch   0 Batch 1260/3125   train_loss = 1.262
2018-04-07T21:44:21.821472: Epoch   0 Batch 1280/3125   train_loss = 1.277
2018-04-07T21:44:23.483909: Epoch   0 Batch 1300/3125   train_loss = 1.239
2018-04-07T21:44:25.144331: Epoch   0 Batch 1320/3125   train_loss = 1.207
2018-04-07T21:44:26.796461: Epoch   0 Batch 1340/3125   train_loss = 1.121
2018-04-07T21:44:28.456646: Epoch   0 Batch 1360/3125   train_loss = 1.179
2018-04-07T21:44:30.125653: Epoch   0 Batch 1380/3125   train_loss = 1.034
2018-04-07T21:44:31.815110: Epoch   0 Batch 1400/3125   train_loss = 1.335
2018-04-07T21:44:33.478947: Epoch   0 Batch 1420/3125   train_loss = 1.282
2018-04-07T21:44:35.153952: Epoch   0 Batch 1440/3125   train_loss = 1.176
2018-04-07T21:44:36.927751: Epoch   0 Batch 1460/3125   train_loss = 1.292
2018-04-07T21:44:38.592249: Epoch   0 Batch 1480/3125   train_loss = 1.251
2018-04-07T21:44:40.265331: Epoch   0 Batch 1500/3125   train_loss = 1.331
2018-04-07T21:44:41.919218: Epoch   0 Batch 1520/3125   train_loss = 1.284
2018-04-07T21:44:43.592286: Epoch   0 Batch 1540/3125   train_loss = 1.324
2018-04-07T21:44:45.272490: Epoch   0 Batch 1560/3125   train_loss = 1.200
2018-04-07T21:44:46.934610: Epoch   0 Batch 1580/3125   train_loss = 1.225
2018-04-07T21:44:48.610956: Epoch   0 Batch 1600/3125   train_loss = 1.288
2018-04-07T21:44:50.270936: Epoch   0 Batch 1620/3125   train_loss = 1.185
2018-04-07T21:44:51.946948: Epoch   0 Batch 1640/3125   train_loss = 1.336
2018-04-07T21:44:53.671031: Epoch   0 Batch 1660/3125   train_loss = 1.342
2018-04-07T21:44:55.361009: Epoch   0 Batch 1680/3125   train_loss = 1.165
2018-04-07T21:44:57.025112: Epoch   0 Batch 1700/3125   train_loss = 1.022
2018-04-07T21:44:58.706588: Epoch   0 Batch 1720/3125   train_loss = 1.165
2018-04-07T21:45:00.357635: Epoch   0 Batch 1740/3125   train_loss = 1.158
2018-04-07T21:45:02.056707: Epoch   0 Batch 1760/3125   train_loss = 1.318
2018-04-07T21:45:03.724830: Epoch   0 Batch 1780/3125   train_loss = 1.113
2018-04-07T21:45:05.388305: Epoch   0 Batch 1800/3125   train_loss = 1.240
2018-04-07T21:45:07.062327: Epoch   0 Batch 1820/3125   train_loss = 1.172
2018-04-07T21:45:08.732785: Epoch   0 Batch 1840/3125   train_loss = 1.340
2018-04-07T21:45:10.517103: Epoch   0 Batch 1860/3125   train_loss = 1.287
2018-04-07T21:45:12.199234: Epoch   0 Batch 1880/3125   train_loss = 1.326
2018-04-07T21:45:13.882208: Epoch   0 Batch 1900/3125   train_loss = 0.988
2018-04-07T21:45:15.551117: Epoch   0 Batch 1920/3125   train_loss = 1.170
2018-04-07T21:45:17.212383: Epoch   0 Batch 1940/3125   train_loss = 1.123
2018-04-07T21:45:18.892947: Epoch   0 Batch 1960/3125   train_loss = 1.145
2018-04-07T21:45:20.568034: Epoch   0 Batch 1980/3125   train_loss = 1.177
2018-04-07T21:45:22.230042: Epoch   0 Batch 2000/3125   train_loss = 1.400
2018-04-07T21:45:23.904087: Epoch   0 Batch 2020/3125   train_loss = 1.294
2018-04-07T21:45:25.572132: Epoch   0 Batch 2040/3125   train_loss = 1.185
2018-04-07T21:45:27.232585: Epoch   0 Batch 2060/3125   train_loss = 1.030
2018-04-07T21:45:28.896008: Epoch   0 Batch 2080/3125   train_loss = 1.299
2018-04-07T21:45:30.589032: Epoch   0 Batch 2100/3125   train_loss = 1.160
2018-04-07T21:45:32.259973: Epoch   0 Batch 2120/3125   train_loss = 1.042
2018-04-07T21:45:33.939778: Epoch   0 Batch 2140/3125   train_loss = 1.140
2018-04-07T21:45:35.751662: Epoch   0 Batch 2160/3125   train_loss = 1.155
2018-04-07T21:45:37.416591: Epoch   0 Batch 2180/3125   train_loss = 1.144
2018-04-07T21:45:39.124136: Epoch   0 Batch 2200/3125   train_loss = 1.161
2018-04-07T21:45:40.804665: Epoch   0 Batch 2220/3125   train_loss = 1.173
2018-04-07T21:45:42.461758: Epoch   0 Batch 2240/3125   train_loss = 1.032
2018-04-07T21:45:44.131229: Epoch   0 Batch 2260/3125   train_loss = 1.123
2018-04-07T21:45:45.789639: Epoch   0 Batch 2280/3125   train_loss = 1.193
2018-04-07T21:45:47.465519: Epoch   0 Batch 2300/3125   train_loss = 1.264
2018-04-07T21:45:49.156554: Epoch   0 Batch 2320/3125   train_loss = 1.377
2018-04-07T21:45:50.843152: Epoch   0 Batch 2340/3125   train_loss = 1.211
2018-04-07T21:45:52.507185: Epoch   0 Batch 2360/3125   train_loss = 1.200
2018-04-07T21:45:54.160133: Epoch   0 Batch 2380/3125   train_loss = 1.165
2018-04-07T21:45:55.819039: Epoch   0 Batch 2400/3125   train_loss = 1.332
2018-04-07T21:45:57.494213: Epoch   0 Batch 2420/3125   train_loss = 1.178
2018-04-07T21:45:59.169702: Epoch   0 Batch 2440/3125   train_loss = 1.291
2018-04-07T21:46:00.848942: Epoch   0 Batch 2460/3125   train_loss = 1.145
2018-04-07T21:46:02.532703: Epoch   0 Batch 2480/3125   train_loss = 1.240
2018-04-07T21:46:04.296052: Epoch   0 Batch 2500/3125   train_loss = 1.267
2018-04-07T21:46:06.021157: Epoch   0 Batch 2520/3125   train_loss = 1.112
2018-04-07T21:46:07.689962: Epoch   0 Batch 2540/3125   train_loss = 1.031
2018-04-07T21:46:09.457103: Epoch   0 Batch 2560/3125   train_loss = 0.962
2018-04-07T21:46:11.136098: Epoch   0 Batch 2580/3125   train_loss = 1.087
2018-04-07T21:46:12.811051: Epoch   0 Batch 2600/3125   train_loss = 1.158
2018-04-07T21:46:14.518089: Epoch   0 Batch 2620/3125   train_loss = 1.097
2018-04-07T21:46:16.209614: Epoch   0 Batch 2640/3125   train_loss = 1.115
2018-04-07T21:46:17.877635: Epoch   0 Batch 2660/3125   train_loss = 1.286
2018-04-07T21:46:19.567628: Epoch   0 Batch 2680/3125   train_loss = 1.047
2018-04-07T21:46:21.225960: Epoch   0 Batch 2700/3125   train_loss = 1.215
2018-04-07T21:46:22.931356: Epoch   0 Batch 2720/3125   train_loss = 1.163
2018-04-07T21:46:24.661469: Epoch   0 Batch 2740/3125   train_loss = 1.147
2018-04-07T21:46:26.434167: Epoch   0 Batch 2760/3125   train_loss = 1.193
2018-04-07T21:46:28.125735: Epoch   0 Batch 2780/3125   train_loss = 1.137
2018-04-07T21:46:29.803222: Epoch   0 Batch 2800/3125   train_loss = 1.453
2018-04-07T21:46:31.476205: Epoch   0 Batch 2820/3125   train_loss = 1.441
2018-04-07T21:46:33.278495: Epoch   0 Batch 2840/3125   train_loss = 1.102
2018-04-07T21:46:34.982187: Epoch   0 Batch 2860/3125   train_loss = 1.107
2018-04-07T21:46:36.632788: Epoch   0 Batch 2880/3125   train_loss = 1.205
2018-04-07T21:46:38.313531: Epoch   0 Batch 2900/3125   train_loss = 1.165
2018-04-07T21:46:39.985003: Epoch   0 Batch 2920/3125   train_loss = 1.216
2018-04-07T21:46:41.664586: Epoch   0 Batch 2940/3125   train_loss = 1.201
2018-04-07T21:46:43.337044: Epoch   0 Batch 2960/3125   train_loss = 1.193
2018-04-07T21:46:45.003475: Epoch   0 Batch 2980/3125   train_loss = 1.165
2018-04-07T21:46:46.664388: Epoch   0 Batch 3000/3125   train_loss = 1.163
2018-04-07T21:46:48.350967: Epoch   0 Batch 3020/3125   train_loss = 1.214
2018-04-07T21:46:50.026677: Epoch   0 Batch 3040/3125   train_loss = 1.149
2018-04-07T21:46:51.710344: Epoch   0 Batch 3060/3125   train_loss = 1.236
2018-04-07T21:46:53.386790: Epoch   0 Batch 3080/3125   train_loss = 1.258
2018-04-07T21:46:55.061249: Epoch   0 Batch 3100/3125   train_loss = 1.209
2018-04-07T21:46:56.748697: Epoch   0 Batch 3120/3125   train_loss = 1.078
2018-04-07T21:46:57.176335: Epoch   0 Batch    0/781   test_loss = 1.007
2018-04-07T21:46:57.504726: Epoch   0 Batch   20/781   test_loss = 1.117
2018-04-07T21:46:57.816054: Epoch   0 Batch   40/781   test_loss = 1.111
2018-04-07T21:46:58.119903: Epoch   0 Batch   60/781   test_loss = 1.325
2018-04-07T21:46:58.424208: Epoch   0 Batch   80/781   test_loss = 1.399
2018-04-07T21:46:58.732027: Epoch   0 Batch  100/781   test_loss = 1.359
2018-04-07T21:46:59.042362: Epoch   0 Batch  120/781   test_loss = 1.247
2018-04-07T21:46:59.355194: Epoch   0 Batch  140/781   test_loss = 1.227
2018-04-07T21:46:59.665017: Epoch   0 Batch  160/781   test_loss = 1.319
2018-04-07T21:46:59.973323: Epoch   0 Batch  180/781   test_loss = 1.214
2018-04-07T21:47:00.277632: Epoch   0 Batch  200/781   test_loss = 1.198
2018-04-07T21:47:00.583947: Epoch   0 Batch  220/781   test_loss = 1.009
2018-04-07T21:47:00.895775: Epoch   0 Batch  240/781   test_loss = 1.151
2018-04-07T21:47:01.215125: Epoch   0 Batch  260/781   test_loss = 1.216
2018-04-07T21:47:01.517937: Epoch   0 Batch  280/781   test_loss = 1.456
2018-04-07T21:47:01.824251: Epoch   0 Batch  300/781   test_loss = 1.173
2018-04-07T21:47:02.130565: Epoch   0 Batch  320/781   test_loss = 1.292
2018-04-07T21:47:02.442394: Epoch   0 Batch  340/781   test_loss = 0.882
2018-04-07T21:47:02.744697: Epoch   0 Batch  360/781   test_loss = 1.332
2018-04-07T21:47:03.051513: Epoch   0 Batch  380/781   test_loss = 1.139
2018-04-07T21:47:03.359832: Epoch   0 Batch  400/781   test_loss = 1.087
2018-04-07T21:47:03.667650: Epoch   0 Batch  420/781   test_loss = 1.030
2018-04-07T21:47:03.978477: Epoch   0 Batch  440/781   test_loss = 1.203
2018-04-07T21:47:04.283789: Epoch   0 Batch  460/781   test_loss = 1.100
2018-04-07T21:47:04.588098: Epoch   0 Batch  480/781   test_loss = 1.167
2018-04-07T21:47:04.893911: Epoch   0 Batch  500/781   test_loss = 0.978
2018-04-07T21:47:05.208123: Epoch   0 Batch  520/781   test_loss = 1.112
2018-04-07T21:47:05.525477: Epoch   0 Batch  540/781   test_loss = 1.042
2018-04-07T21:47:05.833296: Epoch   0 Batch  560/781   test_loss = 1.267
2018-04-07T21:47:06.141616: Epoch   0 Batch  580/781   test_loss = 1.268
2018-04-07T21:47:06.454447: Epoch   0 Batch  600/781   test_loss = 1.216
2018-04-07T21:47:06.761294: Epoch   0 Batch  620/781   test_loss = 1.268
2018-04-07T21:47:07.068611: Epoch   0 Batch  640/781   test_loss = 1.329
2018-04-07T21:47:07.375927: Epoch   0 Batch  660/781   test_loss = 1.155
2018-04-07T21:47:07.696279: Epoch   0 Batch  680/781   test_loss = 1.462
2018-04-07T21:47:08.001590: Epoch   0 Batch  700/781   test_loss = 1.154
2018-04-07T21:47:08.305899: Epoch   0 Batch  720/781   test_loss = 1.309
2018-04-07T21:47:08.615227: Epoch   0 Batch  740/781   test_loss = 1.241
2018-04-07T21:47:08.920038: Epoch   0 Batch  760/781   test_loss = 1.207
2018-04-07T21:47:09.228371: Epoch   0 Batch  780/781   test_loss = 1.214
2018-04-07T21:47:11.222729: Epoch   1 Batch   15/3125   train_loss = 1.195
2018-04-07T21:47:12.890206: Epoch   1 Batch   35/3125   train_loss = 1.178
2018-04-07T21:47:14.563652: Epoch   1 Batch   55/3125   train_loss = 1.237
2018-04-07T21:47:16.239668: Epoch   1 Batch   75/3125   train_loss = 1.118
2018-04-07T21:47:17.923821: Epoch   1 Batch   95/3125   train_loss = 1.029
2018-04-07T21:47:19.714331: Epoch   1 Batch  115/3125   train_loss = 1.216
2018-04-07T21:47:21.396958: Epoch   1 Batch  135/3125   train_loss = 1.013
2018-04-07T21:47:23.088430: Epoch   1 Batch  155/3125   train_loss = 1.057
2018-04-07T21:47:24.757218: Epoch   1 Batch  175/3125   train_loss = 1.071
2018-04-07T21:47:26.447217: Epoch   1 Batch  195/3125   train_loss = 1.228
2018-04-07T21:47:28.116344: Epoch   1 Batch  215/3125   train_loss = 1.157
2018-04-07T21:47:29.813363: Epoch   1 Batch  235/3125   train_loss = 1.083
2018-04-07T21:47:31.482857: Epoch   1 Batch  255/3125   train_loss = 1.247
2018-04-07T21:47:33.156648: Epoch   1 Batch  275/3125   train_loss = 1.062
2018-04-07T21:47:34.841297: Epoch   1 Batch  295/3125   train_loss = 1.085
2018-04-07T21:47:36.508729: Epoch   1 Batch  315/3125   train_loss = 1.033
2018-04-07T21:47:38.183317: Epoch   1 Batch  335/3125   train_loss = 0.995
2018-04-07T21:47:39.856453: Epoch   1 Batch  355/3125   train_loss = 1.141
2018-04-07T21:47:41.509418: Epoch   1 Batch  375/3125   train_loss = 1.127
2018-04-07T21:47:43.173378: Epoch   1 Batch  395/3125   train_loss = 1.079
2018-04-07T21:47:44.968315: Epoch   1 Batch  415/3125   train_loss = 1.290
2018-04-07T21:47:46.701806: Epoch   1 Batch  435/3125   train_loss = 1.130
2018-04-07T21:47:48.937319: Epoch   1 Batch  455/3125   train_loss = 1.113
2018-04-07T21:47:50.904548: Epoch   1 Batch  475/3125   train_loss = 1.154
2018-04-07T21:47:52.583224: Epoch   1 Batch  495/3125   train_loss = 1.124
2018-04-07T21:47:54.247431: Epoch   1 Batch  515/3125   train_loss = 1.103
2018-04-07T21:47:55.924471: Epoch   1 Batch  535/3125   train_loss = 1.135
2018-04-07T21:47:57.639543: Epoch   1 Batch  555/3125   train_loss = 1.211
2018-04-07T21:47:59.321674: Epoch   1 Batch  575/3125   train_loss = 1.215
2018-04-07T21:48:00.989631: Epoch   1 Batch  595/3125   train_loss = 1.307
2018-04-07T21:48:02.663223: Epoch   1 Batch  615/3125   train_loss = 1.061
2018-04-07T21:48:04.501027: Epoch   1 Batch  635/3125   train_loss = 1.157
2018-04-07T21:48:06.169764: Epoch   1 Batch  655/3125   train_loss = 1.026
2018-04-07T21:48:07.848300: Epoch   1 Batch  675/3125   train_loss = 0.948
2018-04-07T21:48:09.554302: Epoch   1 Batch  695/3125   train_loss = 1.093
2018-04-07T21:48:11.250845: Epoch   1 Batch  715/3125   train_loss = 1.157
2018-04-07T21:48:12.936359: Epoch   1 Batch  735/3125   train_loss = 0.994
2018-04-07T21:48:14.612167: Epoch   1 Batch  755/3125   train_loss = 1.166
2018-04-07T21:48:16.278097: Epoch   1 Batch  775/3125   train_loss = 0.971
2018-04-07T21:48:17.950377: Epoch   1 Batch  795/3125   train_loss = 1.202
2018-04-07T21:48:19.747572: Epoch   1 Batch  815/3125   train_loss = 1.213
2018-04-07T21:48:21.424079: Epoch   1 Batch  835/3125   train_loss = 1.064
2018-04-07T21:48:23.100531: Epoch   1 Batch  855/3125   train_loss = 1.371
2018-04-07T21:48:24.774482: Epoch   1 Batch  875/3125   train_loss = 1.244
2018-04-07T21:48:26.455718: Epoch   1 Batch  895/3125   train_loss = 1.051
2018-04-07T21:48:28.153808: Epoch   1 Batch  915/3125   train_loss = 1.127
2018-04-07T21:48:29.993912: Epoch   1 Batch  935/3125   train_loss = 1.244
2018-04-07T21:48:32.137153: Epoch   1 Batch  955/3125   train_loss = 1.154
2018-04-07T21:48:33.849262: Epoch   1 Batch  975/3125   train_loss = 1.082
2018-04-07T21:48:35.517662: Epoch   1 Batch  995/3125   train_loss = 0.875
2018-04-07T21:48:37.207198: Epoch   1 Batch 1015/3125   train_loss = 1.158
2018-04-07T21:48:38.878117: Epoch   1 Batch 1035/3125   train_loss = 1.119
2018-04-07T21:48:40.582146: Epoch   1 Batch 1055/3125   train_loss = 1.145
2018-04-07T21:48:42.247609: Epoch   1 Batch 1075/3125   train_loss = 1.070
2018-04-07T21:48:44.065942: Epoch   1 Batch 1095/3125   train_loss = 1.052
2018-04-07T21:48:45.747877: Epoch   1 Batch 1115/3125   train_loss = 1.045
2018-04-07T21:48:47.427603: Epoch   1 Batch 1135/3125   train_loss = 1.047
2018-04-07T21:48:49.075965: Epoch   1 Batch 1155/3125   train_loss = 1.116
2018-04-07T21:48:50.738437: Epoch   1 Batch 1175/3125   train_loss = 1.101
2018-04-07T21:48:52.403065: Epoch   1 Batch 1195/3125   train_loss = 1.275
2018-04-07T21:48:54.062049: Epoch   1 Batch 1215/3125   train_loss = 0.992
2018-04-07T21:48:55.737969: Epoch   1 Batch 1235/3125   train_loss = 1.112
2018-04-07T21:48:57.400475: Epoch   1 Batch 1255/3125   train_loss = 1.057
2018-04-07T21:48:59.079432: Epoch   1 Batch 1275/3125   train_loss = 0.953
2018-04-07T21:49:00.742903: Epoch   1 Batch 1295/3125   train_loss = 1.031
2018-04-07T21:49:02.425370: Epoch   1 Batch 1315/3125   train_loss = 1.213
2018-04-07T21:49:04.086209: Epoch   1 Batch 1335/3125   train_loss = 1.046
2018-04-07T21:49:05.760917: Epoch   1 Batch 1355/3125   train_loss = 1.096
2018-04-07T21:49:07.447556: Epoch   1 Batch 1375/3125   train_loss = 1.154
2018-04-07T21:49:09.123011: Epoch   1 Batch 1395/3125   train_loss = 1.107
2018-04-07T21:49:10.800102: Epoch   1 Batch 1415/3125   train_loss = 1.181
2018-04-07T21:49:12.466042: Epoch   1 Batch 1435/3125   train_loss = 1.193
2018-04-07T21:49:14.128461: Epoch   1 Batch 1455/3125   train_loss = 1.229
2018-04-07T21:49:15.797421: Epoch   1 Batch 1475/3125   train_loss = 1.066
2018-04-07T21:49:17.471934: Epoch   1 Batch 1495/3125   train_loss = 1.045
2018-04-07T21:49:19.244979: Epoch   1 Batch 1515/3125   train_loss = 0.973
2018-04-07T21:49:20.932703: Epoch   1 Batch 1535/3125   train_loss = 0.921
2018-04-07T21:49:22.707473: Epoch   1 Batch 1555/3125   train_loss = 1.077
2018-04-07T21:49:24.360143: Epoch   1 Batch 1575/3125   train_loss = 1.082
2018-04-07T21:49:26.031133: Epoch   1 Batch 1595/3125   train_loss = 1.130
2018-04-07T21:49:27.702657: Epoch   1 Batch 1615/3125   train_loss = 1.108
2018-04-07T21:49:29.371590: Epoch   1 Batch 1635/3125   train_loss = 1.068
2018-04-07T21:49:31.040220: Epoch   1 Batch 1655/3125   train_loss = 1.159
2018-04-07T21:49:32.719263: Epoch   1 Batch 1675/3125   train_loss = 1.062
2018-04-07T21:49:34.393526: Epoch   1 Batch 1695/3125   train_loss = 1.020
2018-04-07T21:49:36.054509: Epoch   1 Batch 1715/3125   train_loss = 0.944
2018-04-07T21:49:37.719712: Epoch   1 Batch 1735/3125   train_loss = 1.221
2018-04-07T21:49:39.366590: Epoch   1 Batch 1755/3125   train_loss = 1.028
2018-04-07T21:49:41.040912: Epoch   1 Batch 1775/3125   train_loss = 1.032
2018-04-07T21:49:42.805265: Epoch   1 Batch 1795/3125   train_loss = 1.056
2018-04-07T21:49:44.475704: Epoch   1 Batch 1815/3125   train_loss = 1.053
2018-04-07T21:49:46.148219: Epoch   1 Batch 1835/3125   train_loss = 1.174
2018-04-07T21:49:47.828801: Epoch   1 Batch 1855/3125   train_loss = 0.906
2018-04-07T21:49:49.509882: Epoch   1 Batch 1875/3125   train_loss = 1.219
2018-04-07T21:49:51.168636: Epoch   1 Batch 1895/3125   train_loss = 0.989
2018-04-07T21:49:52.847100: Epoch   1 Batch 1915/3125   train_loss = 0.950
2018-04-07T21:49:54.512075: Epoch   1 Batch 1935/3125   train_loss = 0.965
2018-04-07T21:49:56.192026: Epoch   1 Batch 1955/3125   train_loss = 1.053
2018-04-07T21:49:57.851496: Epoch   1 Batch 1975/3125   train_loss = 1.087
2018-04-07T21:49:59.541074: Epoch   1 Batch 1995/3125   train_loss = 1.228
2018-04-07T21:50:01.220183: Epoch   1 Batch 2015/3125   train_loss = 1.082
2018-04-07T21:50:02.917751: Epoch   1 Batch 2035/3125   train_loss = 1.168
2018-04-07T21:50:04.709645: Epoch   1 Batch 2055/3125   train_loss = 1.050
2018-04-07T21:50:06.372288: Epoch   1 Batch 2075/3125   train_loss = 1.131
2018-04-07T21:50:08.055763: Epoch   1 Batch 2095/3125   train_loss = 1.038
2018-04-07T21:50:09.737262: Epoch   1 Batch 2115/3125   train_loss = 1.149
2018-04-07T21:50:11.406194: Epoch   1 Batch 2135/3125   train_loss = 1.009
2018-04-07T21:50:13.084656: Epoch   1 Batch 2155/3125   train_loss = 1.036
2018-04-07T21:50:14.752650: Epoch   1 Batch 2175/3125   train_loss = 1.042
2018-04-07T21:50:16.545914: Epoch   1 Batch 2195/3125   train_loss = 1.030
2018-04-07T21:50:18.267374: Epoch   1 Batch 2215/3125   train_loss = 1.061
2018-04-07T21:50:19.929044: Epoch   1 Batch 2235/3125   train_loss = 1.201
2018-04-07T21:50:21.596054: Epoch   1 Batch 2255/3125   train_loss = 1.178
2018-04-07T21:50:23.277873: Epoch   1 Batch 2275/3125   train_loss = 0.919
2018-04-07T21:50:24.960918: Epoch   1 Batch 2295/3125   train_loss = 1.279
2018-04-07T21:50:26.630553: Epoch   1 Batch 2315/3125   train_loss = 1.151
2018-04-07T21:50:28.297282: Epoch   1 Batch 2335/3125   train_loss = 1.062
2018-04-07T21:50:29.989930: Epoch   1 Batch 2355/3125   train_loss = 1.030
2018-04-07T21:50:31.663946: Epoch   1 Batch 2375/3125   train_loss = 1.285
2018-04-07T21:50:33.329603: Epoch   1 Batch 2395/3125   train_loss = 1.027
2018-04-07T21:50:34.989065: Epoch   1 Batch 2415/3125   train_loss = 1.106
2018-04-07T21:50:36.660182: Epoch   1 Batch 2435/3125   train_loss = 0.971
2018-04-07T21:50:38.324022: Epoch   1 Batch 2455/3125   train_loss = 1.085
2018-04-07T21:50:40.100051: Epoch   1 Batch 2475/3125   train_loss = 1.012
2018-04-07T21:50:41.779648: Epoch   1 Batch 2495/3125   train_loss = 0.967
2018-04-07T21:50:43.476129: Epoch   1 Batch 2515/3125   train_loss = 1.061
2018-04-07T21:50:45.151622: Epoch   1 Batch 2535/3125   train_loss = 1.039
2018-04-07T21:50:46.837221: Epoch   1 Batch 2555/3125   train_loss = 0.914
2018-04-07T21:50:48.534534: Epoch   1 Batch 2575/3125   train_loss = 0.978
2018-04-07T21:50:50.202346: Epoch   1 Batch 2595/3125   train_loss = 1.040
2018-04-07T21:50:51.934019: Epoch   1 Batch 2615/3125   train_loss = 1.171
2018-04-07T21:50:53.605018: Epoch   1 Batch 2635/3125   train_loss = 0.946
2018-04-07T21:50:55.297450: Epoch   1 Batch 2655/3125   train_loss = 1.037
2018-04-07T21:50:56.959585: Epoch   1 Batch 2675/3125   train_loss = 0.966
2018-04-07T21:50:58.630080: Epoch   1 Batch 2695/3125   train_loss = 1.068
2018-04-07T21:51:00.308062: Epoch   1 Batch 2715/3125   train_loss = 1.003
2018-04-07T21:51:01.997950: Epoch   1 Batch 2735/3125   train_loss = 0.872
2018-04-07T21:51:03.689997: Epoch   1 Batch 2755/3125   train_loss = 1.123
2018-04-07T21:51:05.360402: Epoch   1 Batch 2775/3125   train_loss = 1.097
2018-04-07T21:51:07.059475: Epoch   1 Batch 2795/3125   train_loss = 1.107
2018-04-07T21:51:08.724060: Epoch   1 Batch 2815/3125   train_loss = 0.954
2018-04-07T21:51:10.408532: Epoch   1 Batch 2835/3125   train_loss = 1.100
2018-04-07T21:51:12.075182: Epoch   1 Batch 2855/3125   train_loss = 1.140
2018-04-07T21:51:13.754898: Epoch   1 Batch 2875/3125   train_loss = 0.990
2018-04-07T21:51:15.538883: Epoch   1 Batch 2895/3125   train_loss = 1.102
2018-04-07T21:51:17.220206: Epoch   1 Batch 2915/3125   train_loss = 0.925
2018-04-07T21:51:18.900634: Epoch   1 Batch 2935/3125   train_loss = 1.125
2018-04-07T21:51:20.584828: Epoch   1 Batch 2955/3125   train_loss = 1.122
2018-04-07T21:51:22.267826: Epoch   1 Batch 2975/3125   train_loss = 0.999
2018-04-07T21:51:23.953166: Epoch   1 Batch 2995/3125   train_loss = 1.029
2018-04-07T21:51:25.648223: Epoch   1 Batch 3015/3125   train_loss = 0.952
2018-04-07T21:51:27.338531: Epoch   1 Batch 3035/3125   train_loss = 0.997
2018-04-07T21:51:29.023311: Epoch   1 Batch 3055/3125   train_loss = 1.082
2018-04-07T21:51:30.702958: Epoch   1 Batch 3075/3125   train_loss = 0.946
2018-04-07T21:51:32.383737: Epoch   1 Batch 3095/3125   train_loss = 0.980
2018-04-07T21:51:34.082951: Epoch   1 Batch 3115/3125   train_loss = 0.906
2018-04-07T21:51:35.214765: Epoch   1 Batch   19/781   test_loss = 0.972
2018-04-07T21:51:35.522580: Epoch   1 Batch   39/781   test_loss = 0.903
2018-04-07T21:51:35.832405: Epoch   1 Batch   59/781   test_loss = 0.935
2018-04-07T21:51:36.138718: Epoch   1 Batch   79/781   test_loss = 1.007
2018-04-07T21:51:36.437976: Epoch   1 Batch   99/781   test_loss = 1.025
2018-04-07T21:51:36.761336: Epoch   1 Batch  119/781   test_loss = 1.019
2018-04-07T21:51:37.068649: Epoch   1 Batch  139/781   test_loss = 1.098
2018-04-07T21:51:37.375468: Epoch   1 Batch  159/781   test_loss = 1.031
2018-04-07T21:51:37.679302: Epoch   1 Batch  179/781   test_loss = 0.963
2018-04-07T21:51:37.982608: Epoch   1 Batch  199/781   test_loss = 0.930
2018-04-07T21:51:38.289925: Epoch   1 Batch  219/781   test_loss = 1.034
2018-04-07T21:51:38.589228: Epoch   1 Batch  239/781   test_loss = 1.183
2018-04-07T21:51:38.903571: Epoch   1 Batch  259/781   test_loss = 1.029
2018-04-07T21:51:39.204872: Epoch   1 Batch  279/781   test_loss = 1.143
2018-04-07T21:51:39.506174: Epoch   1 Batch  299/781   test_loss = 1.237
2018-04-07T21:51:39.815997: Epoch   1 Batch  319/781   test_loss = 0.976
2018-04-07T21:51:40.120306: Epoch   1 Batch  339/781   test_loss = 0.896
2018-04-07T21:51:40.421107: Epoch   1 Batch  359/781   test_loss = 0.962
2018-04-07T21:51:40.722412: Epoch   1 Batch  379/781   test_loss = 1.045
2018-04-07T21:51:41.031250: Epoch   1 Batch  399/781   test_loss = 0.873
2018-04-07T21:51:41.334055: Epoch   1 Batch  419/781   test_loss = 0.968
2018-04-07T21:51:41.635857: Epoch   1 Batch  439/781   test_loss = 1.011
2018-04-07T21:51:41.943172: Epoch   1 Batch  459/781   test_loss = 1.112
2018-04-07T21:51:42.249989: Epoch   1 Batch  479/781   test_loss = 1.052
2018-04-07T21:51:42.551308: Epoch   1 Batch  499/781   test_loss = 0.934
2018-04-07T21:51:42.853111: Epoch   1 Batch  519/781   test_loss = 1.145
2018-04-07T21:51:43.153408: Epoch   1 Batch  539/781   test_loss = 0.856
2018-04-07T21:51:43.455211: Epoch   1 Batch  559/781   test_loss = 1.120
2018-04-07T21:51:43.762027: Epoch   1 Batch  579/781   test_loss = 1.058
2018-04-07T21:51:44.073354: Epoch   1 Batch  599/781   test_loss = 0.974
2018-04-07T21:51:44.377662: Epoch   1 Batch  619/781   test_loss = 1.131
2018-04-07T21:51:44.685982: Epoch   1 Batch  639/781   test_loss = 0.900
2018-04-07T21:51:44.997309: Epoch   1 Batch  659/781   test_loss = 1.166
2018-04-07T21:51:45.299613: Epoch   1 Batch  679/781   test_loss = 1.192
2018-04-07T21:51:45.600413: Epoch   1 Batch  699/781   test_loss = 0.905
2018-04-07T21:51:45.904219: Epoch   1 Batch  719/781   test_loss = 1.007
2018-04-07T21:51:46.206022: Epoch   1 Batch  739/781   test_loss = 0.963
2018-04-07T21:51:46.511835: Epoch   1 Batch  759/781   test_loss = 0.933
2018-04-07T21:51:46.816146: Epoch   1 Batch  779/781   test_loss = 0.827
2018-04-07T21:51:48.364830: Epoch   2 Batch   10/3125   train_loss = 0.980
2018-04-07T21:51:50.043889: Epoch   2 Batch   30/3125   train_loss = 1.055
2018-04-07T21:51:51.839307: Epoch   2 Batch   50/3125   train_loss = 1.091
2018-04-07T21:51:53.495546: Epoch   2 Batch   70/3125   train_loss = 1.055
2018-04-07T21:51:55.188746: Epoch   2 Batch   90/3125   train_loss = 0.969
2018-04-07T21:51:56.850718: Epoch   2 Batch  110/3125   train_loss = 0.950
2018-04-07T21:51:58.524022: Epoch   2 Batch  130/3125   train_loss = 0.988
2018-04-07T21:52:00.192710: Epoch   2 Batch  150/3125   train_loss = 1.122
2018-04-07T21:52:01.866708: Epoch   2 Batch  170/3125   train_loss = 1.086
2018-04-07T21:52:03.556374: Epoch   2 Batch  190/3125   train_loss = 1.098
2018-04-07T21:52:05.268324: Epoch   2 Batch  210/3125   train_loss = 1.002
2018-04-07T21:52:06.937866: Epoch   2 Batch  230/3125   train_loss = 1.061
2018-04-07T21:52:08.625362: Epoch   2 Batch  250/3125   train_loss = 0.947
2018-04-07T21:52:10.312387: Epoch   2 Batch  270/3125   train_loss = 0.835
2018-04-07T21:52:12.016290: Epoch   2 Batch  290/3125   train_loss = 1.055
2018-04-07T21:52:13.695074: Epoch   2 Batch  310/3125   train_loss = 0.988
2018-04-07T21:52:15.375322: Epoch   2 Batch  330/3125   train_loss = 1.034
2018-04-07T21:52:17.052781: Epoch   2 Batch  350/3125   train_loss = 0.900
2018-04-07T21:52:18.718326: Epoch   2 Batch  370/3125   train_loss = 1.174
2018-04-07T21:52:20.376276: Epoch   2 Batch  390/3125   train_loss = 1.179
2018-04-07T21:52:22.059794: Epoch   2 Batch  410/3125   train_loss = 0.935
2018-04-07T21:52:23.745309: Epoch   2 Batch  430/3125   train_loss = 1.154
2018-04-07T21:52:25.546226: Epoch   2 Batch  450/3125   train_loss = 0.902
2018-04-07T21:52:27.219194: Epoch   2 Batch  470/3125   train_loss = 0.998
2018-04-07T21:52:28.899160: Epoch   2 Batch  490/3125   train_loss = 0.997
2018-04-07T21:52:30.591875: Epoch   2 Batch  510/3125   train_loss = 1.106
2018-04-07T21:52:32.274955: Epoch   2 Batch  530/3125   train_loss = 0.918
2018-04-07T21:52:33.967771: Epoch   2 Batch  550/3125   train_loss = 1.065
2018-04-07T21:52:35.645019: Epoch   2 Batch  570/3125   train_loss = 1.154
2018-04-07T21:52:37.321502: Epoch   2 Batch  590/3125   train_loss = 0.964
2018-04-07T21:52:39.037773: Epoch   2 Batch  610/3125   train_loss = 1.024
2018-04-07T21:52:40.734933: Epoch   2 Batch  630/3125   train_loss = 1.035
2018-04-07T21:52:42.417596: Epoch   2 Batch  650/3125   train_loss = 1.010
2018-04-07T21:52:44.113104: Epoch   2 Batch  670/3125   train_loss = 1.028
2018-04-07T21:52:45.833174: Epoch   2 Batch  690/3125   train_loss = 0.935
2018-04-07T21:52:47.505182: Epoch   2 Batch  710/3125   train_loss = 0.963
2018-04-07T21:52:49.289961: Epoch   2 Batch  730/3125   train_loss = 0.851
2018-04-07T21:52:50.966444: Epoch   2 Batch  750/3125   train_loss = 0.970
2018-04-07T21:52:52.660569: Epoch   2 Batch  770/3125   train_loss = 0.884
2018-04-07T21:52:54.333481: Epoch   2 Batch  790/3125   train_loss = 0.892
2018-04-07T21:52:56.011105: Epoch   2 Batch  810/3125   train_loss = 0.875
2018-04-07T21:52:57.693592: Epoch   2 Batch  830/3125   train_loss = 0.898
2018-04-07T21:52:59.358190: Epoch   2 Batch  850/3125   train_loss = 0.952
2018-04-07T21:53:01.025612: Epoch   2 Batch  870/3125   train_loss = 0.947
2018-04-07T21:53:02.704187: Epoch   2 Batch  890/3125   train_loss = 0.931
2018-04-07T21:53:04.387258: Epoch   2 Batch  910/3125   train_loss = 0.985
2018-04-07T21:53:06.070769: Epoch   2 Batch  930/3125   train_loss = 0.994
2018-04-07T21:53:07.768316: Epoch   2 Batch  950/3125   train_loss = 0.895
2018-04-07T21:53:09.432223: Epoch   2 Batch  970/3125   train_loss = 1.104
2018-04-07T21:53:11.108773: Epoch   2 Batch  990/3125   train_loss = 0.923
2018-04-07T21:53:12.797316: Epoch   2 Batch 1010/3125   train_loss = 1.147
2018-04-07T21:53:14.470768: Epoch   2 Batch 1030/3125   train_loss = 0.992
2018-04-07T21:53:16.149782: Epoch   2 Batch 1050/3125   train_loss = 0.958
2018-04-07T21:53:17.813705: Epoch   2 Batch 1070/3125   train_loss = 0.997
2018-04-07T21:53:19.503161: Epoch   2 Batch 1090/3125   train_loss = 1.085
2018-04-07T21:53:21.176699: Epoch   2 Batch 1110/3125   train_loss = 1.051
2018-04-07T21:53:22.855660: Epoch   2 Batch 1130/3125   train_loss = 1.038
2018-04-07T21:53:24.650896: Epoch   2 Batch 1150/3125   train_loss = 0.988
2018-04-07T21:53:26.311343: Epoch   2 Batch 1170/3125   train_loss = 0.945
2018-04-07T21:53:27.983129: Epoch   2 Batch 1190/3125   train_loss = 1.042
2018-04-07T21:53:29.654150: Epoch   2 Batch 1210/3125   train_loss = 0.940
2018-04-07T21:53:31.344711: Epoch   2 Batch 1230/3125   train_loss = 0.877
2018-04-07T21:53:33.024855: Epoch   2 Batch 1250/3125   train_loss = 0.997
2018-04-07T21:53:34.714176: Epoch   2 Batch 1270/3125   train_loss = 1.004
2018-04-07T21:53:36.397614: Epoch   2 Batch 1290/3125   train_loss = 0.943
2018-04-07T21:53:38.094978: Epoch   2 Batch 1310/3125   train_loss = 0.987
2018-04-07T21:53:39.801519: Epoch   2 Batch 1330/3125   train_loss = 1.127
2018-04-07T21:53:41.472926: Epoch   2 Batch 1350/3125   train_loss = 0.886
2018-04-07T21:53:43.152917: Epoch   2 Batch 1370/3125   train_loss = 0.893
2018-04-07T21:53:44.844412: Epoch   2 Batch 1390/3125   train_loss = 1.021
2018-04-07T21:53:46.527452: Epoch   2 Batch 1410/3125   train_loss = 0.968
2018-04-07T21:53:48.305398: Epoch   2 Batch 1430/3125   train_loss = 0.996
2018-04-07T21:53:49.990926: Epoch   2 Batch 1450/3125   train_loss = 1.002
2018-04-07T21:53:51.697686: Epoch   2 Batch 1470/3125   train_loss = 0.995
2018-04-07T21:53:53.389343: Epoch   2 Batch 1490/3125   train_loss = 1.014
2018-04-07T21:53:55.065369: Epoch   2 Batch 1510/3125   train_loss = 1.002
2018-04-07T21:53:56.747512: Epoch   2 Batch 1530/3125   train_loss = 1.092
2018-04-07T21:53:58.433653: Epoch   2 Batch 1550/3125   train_loss = 0.901
2018-04-07T21:54:00.120763: Epoch   2 Batch 1570/3125   train_loss = 0.971
2018-04-07T21:54:01.800792: Epoch   2 Batch 1590/3125   train_loss = 0.962
2018-04-07T21:54:03.489304: Epoch   2 Batch 1610/3125   train_loss = 1.060
2018-04-07T21:54:05.233587: Epoch   2 Batch 1630/3125   train_loss = 1.070
2018-04-07T21:54:06.904447: Epoch   2 Batch 1650/3125   train_loss = 0.851
2018-04-07T21:54:08.586122: Epoch   2 Batch 1670/3125   train_loss = 0.851
2018-04-07T21:54:10.274675: Epoch   2 Batch 1690/3125   train_loss = 1.001
2018-04-07T21:54:11.946220: Epoch   2 Batch 1710/3125   train_loss = 0.982
2018-04-07T21:54:13.622559: Epoch   2 Batch 1730/3125   train_loss = 0.986
2018-04-07T21:54:15.302490: Epoch   2 Batch 1750/3125   train_loss = 0.829
2018-04-07T21:54:16.991875: Epoch   2 Batch 1770/3125   train_loss = 1.106
2018-04-07T21:54:18.675352: Epoch   2 Batch 1790/3125   train_loss = 0.980
2018-04-07T21:54:20.362342: Epoch   2 Batch 1810/3125   train_loss = 0.977
2018-04-07T21:54:22.139171: Epoch   2 Batch 1830/3125   train_loss = 1.051
2018-04-07T21:54:23.831192: Epoch   2 Batch 1850/3125   train_loss = 0.975
2018-04-07T21:54:25.510260: Epoch   2 Batch 1870/3125   train_loss = 0.992
2018-04-07T21:54:27.198247: Epoch   2 Batch 1890/3125   train_loss = 0.798
2018-04-07T21:54:28.873822: Epoch   2 Batch 1910/3125   train_loss = 0.887
2018-04-07T21:54:30.566315: Epoch   2 Batch 1930/3125   train_loss = 1.035
2018-04-07T21:54:32.245806: Epoch   2 Batch 1950/3125   train_loss = 0.812
2018-04-07T21:54:33.922406: Epoch   2 Batch 1970/3125   train_loss = 0.975
2018-04-07T21:54:35.606341: Epoch   2 Batch 1990/3125   train_loss = 0.889
2018-04-07T21:54:37.295885: Epoch   2 Batch 2010/3125   train_loss = 0.825
2018-04-07T21:54:38.971337: Epoch   2 Batch 2030/3125   train_loss = 0.937
2018-04-07T21:54:40.639443: Epoch   2 Batch 2050/3125   train_loss = 0.940
2018-04-07T21:54:42.318935: Epoch   2 Batch 2070/3125   train_loss = 0.907
2018-04-07T21:54:44.010273: Epoch   2 Batch 2090/3125   train_loss = 0.906
2018-04-07T21:54:45.811586: Epoch   2 Batch 2110/3125   train_loss = 1.106
2018-04-07T21:54:47.497570: Epoch   2 Batch 2130/3125   train_loss = 0.973
2018-04-07T21:54:49.168031: Epoch   2 Batch 2150/3125   train_loss = 1.012
2018-04-07T21:54:50.851152: Epoch   2 Batch 2170/3125   train_loss = 0.873
2018-04-07T21:54:52.554718: Epoch   2 Batch 2190/3125   train_loss = 0.958
2018-04-07T21:54:54.234770: Epoch   2 Batch 2210/3125   train_loss = 0.982
2018-04-07T21:54:55.908706: Epoch   2 Batch 2230/3125   train_loss = 0.868
2018-04-07T21:54:57.658857: Epoch   2 Batch 2250/3125   train_loss = 1.009
2018-04-07T21:54:59.333929: Epoch   2 Batch 2270/3125   train_loss = 0.985
2018-04-07T21:55:01.032484: Epoch   2 Batch 2290/3125   train_loss = 0.772
2018-04-07T21:55:02.712918: Epoch   2 Batch 2310/3125   train_loss = 0.898
2018-04-07T21:55:04.389962: Epoch   2 Batch 2330/3125   train_loss = 1.057
2018-04-07T21:55:06.066463: Epoch   2 Batch 2350/3125   train_loss = 0.954
2018-04-07T21:55:07.757504: Epoch   2 Batch 2370/3125   train_loss = 0.911
2018-04-07T21:55:09.454013: Epoch   2 Batch 2390/3125   train_loss = 0.999
2018-04-07T21:55:11.136171: Epoch   2 Batch 2410/3125   train_loss = 1.040
2018-04-07T21:55:12.824430: Epoch   2 Batch 2430/3125   train_loss = 0.899
2018-04-07T21:55:14.503012: Epoch   2 Batch 2450/3125   train_loss = 0.988
2018-04-07T21:55:16.191371: Epoch   2 Batch 2470/3125   train_loss = 1.007
2018-04-07T21:55:17.897583: Epoch   2 Batch 2490/3125   train_loss = 1.058
2018-04-07T21:55:19.590753: Epoch   2 Batch 2510/3125   train_loss = 1.022
2018-04-07T21:55:21.409135: Epoch   2 Batch 2530/3125   train_loss = 0.776
2018-04-07T21:55:23.111442: Epoch   2 Batch 2550/3125   train_loss = 1.015
2018-04-07T21:55:24.796901: Epoch   2 Batch 2570/3125   train_loss = 1.047
2018-04-07T21:55:26.490478: Epoch   2 Batch 2590/3125   train_loss = 1.001
2018-04-07T21:55:28.200558: Epoch   2 Batch 2610/3125   train_loss = 0.974
2018-04-07T21:55:29.867284: Epoch   2 Batch 2630/3125   train_loss = 0.684
2018-04-07T21:55:31.536244: Epoch   2 Batch 2650/3125   train_loss = 0.909
2018-04-07T21:55:33.212675: Epoch   2 Batch 2670/3125   train_loss = 0.990
2018-04-07T21:55:34.898188: Epoch   2 Batch 2690/3125   train_loss = 0.934
2018-04-07T21:55:36.569125: Epoch   2 Batch 2710/3125   train_loss = 0.850
2018-04-07T21:55:38.251205: Epoch   2 Batch 2730/3125   train_loss = 1.057
2018-04-07T21:55:39.939942: Epoch   2 Batch 2750/3125   train_loss = 1.017
2018-04-07T21:55:41.616899: Epoch   2 Batch 2770/3125   train_loss = 0.923
2018-04-07T21:55:43.355018: Epoch   2 Batch 2790/3125   train_loss = 0.896
2018-04-07T21:55:45.159178: Epoch   2 Batch 2810/3125   train_loss = 0.965
2018-04-07T21:55:46.855776: Epoch   2 Batch 2830/3125   train_loss = 0.867
2018-04-07T21:55:48.540434: Epoch   2 Batch 2850/3125   train_loss = 0.971
2018-04-07T21:55:50.225449: Epoch   2 Batch 2870/3125   train_loss = 0.818
2018-04-07T21:55:51.986597: Epoch   2 Batch 2890/3125   train_loss = 0.850
2018-04-07T21:55:53.709714: Epoch   2 Batch 2910/3125   train_loss = 0.946
2018-04-07T21:55:55.437403: Epoch   2 Batch 2930/3125   train_loss = 0.811
2018-04-07T21:55:57.127661: Epoch   2 Batch 2950/3125   train_loss = 1.126
2018-04-07T21:55:58.817648: Epoch   2 Batch 2970/3125   train_loss = 0.930
2018-04-07T21:56:00.537274: Epoch   2 Batch 2990/3125   train_loss = 0.909
2018-04-07T21:56:02.235055: Epoch   2 Batch 3010/3125   train_loss = 0.939
2018-04-07T21:56:03.987710: Epoch   2 Batch 3030/3125   train_loss = 0.894
2018-04-07T21:56:05.765976: Epoch   2 Batch 3050/3125   train_loss = 0.914
2018-04-07T21:56:07.548016: Epoch   2 Batch 3070/3125   train_loss = 0.881
2018-04-07T21:56:09.214199: Epoch   2 Batch 3090/3125   train_loss = 0.825
2018-04-07T21:56:10.907201: Epoch   2 Batch 3110/3125   train_loss = 0.867
2018-04-07T21:56:12.442591: Epoch   2 Batch   18/781   test_loss = 0.784
2018-04-07T21:56:12.746398: Epoch   2 Batch   38/781   test_loss = 0.870
2018-04-07T21:56:13.051209: Epoch   2 Batch   58/781   test_loss = 0.896
2018-04-07T21:56:13.353011: Epoch   2 Batch   78/781   test_loss = 0.880
2018-04-07T21:56:13.658312: Epoch   2 Batch   98/781   test_loss = 0.943
2018-04-07T21:56:13.972145: Epoch   2 Batch  118/781   test_loss = 0.812
2018-04-07T21:56:14.283976: Epoch   2 Batch  138/781   test_loss = 1.006
2018-04-07T21:56:14.590289: Epoch   2 Batch  158/781   test_loss = 0.919
2018-04-07T21:56:14.894097: Epoch   2 Batch  178/781   test_loss = 0.913
2018-04-07T21:56:15.201914: Epoch   2 Batch  198/781   test_loss = 0.836
2018-04-07T21:56:15.513745: Epoch   2 Batch  218/781   test_loss = 1.059
2018-04-07T21:56:15.825588: Epoch   2 Batch  238/781   test_loss = 0.973
2018-04-07T21:56:16.135912: Epoch   2 Batch  258/781   test_loss = 1.062
2018-04-07T21:56:16.438718: Epoch   2 Batch  278/781   test_loss = 1.065
2018-04-07T21:56:16.747038: Epoch   2 Batch  298/781   test_loss = 0.923
2018-04-07T21:56:17.054855: Epoch   2 Batch  318/781   test_loss = 0.877
2018-04-07T21:56:17.363188: Epoch   2 Batch  338/781   test_loss = 0.957
2018-04-07T21:56:17.664983: Epoch   2 Batch  358/781   test_loss = 0.933
2018-04-07T21:56:17.967821: Epoch   2 Batch  378/781   test_loss = 0.888
2018-04-07T21:56:18.275139: Epoch   2 Batch  398/781   test_loss = 0.800
2018-04-07T21:56:18.582454: Epoch   2 Batch  418/781   test_loss = 1.006
2018-04-07T21:56:18.889772: Epoch   2 Batch  438/781   test_loss = 1.008
2018-04-07T21:56:19.198090: Epoch   2 Batch  458/781   test_loss = 0.909
2018-04-07T21:56:19.500896: Epoch   2 Batch  478/781   test_loss = 0.966
2018-04-07T21:56:19.803700: Epoch   2 Batch  498/781   test_loss = 0.818
2018-04-07T21:56:20.098986: Epoch   2 Batch  518/781   test_loss = 0.989
2018-04-07T21:56:20.405813: Epoch   2 Batch  538/781   test_loss = 0.879
2018-04-07T21:56:20.709620: Epoch   2 Batch  558/781   test_loss = 0.866
2018-04-07T21:56:21.019945: Epoch   2 Batch  578/781   test_loss = 0.918
2018-04-07T21:56:21.342301: Epoch   2 Batch  598/781   test_loss = 1.033
2018-04-07T21:56:21.644629: Epoch   2 Batch  618/781   test_loss = 0.885
2018-04-07T21:56:21.947936: Epoch   2 Batch  638/781   test_loss = 0.851
2018-04-07T21:56:22.247231: Epoch   2 Batch  658/781   test_loss = 1.024
2018-04-07T21:56:22.565596: Epoch   2 Batch  678/781   test_loss = 0.918
2018-04-07T21:56:22.865894: Epoch   2 Batch  698/781   test_loss = 0.866
2018-04-07T21:56:23.172710: Epoch   2 Batch  718/781   test_loss = 0.982
2018-04-07T21:56:23.477019: Epoch   2 Batch  738/781   test_loss = 0.889
2018-04-07T21:56:23.781328: Epoch   2 Batch  758/781   test_loss = 0.901
2018-04-07T21:56:24.096666: Epoch   2 Batch  778/781   test_loss = 0.903
2018-04-07T21:56:25.282361: Epoch   3 Batch    5/3125   train_loss = 0.960
2018-04-07T21:56:27.001355: Epoch   3 Batch   25/3125   train_loss = 1.070
2018-04-07T21:56:28.744986: Epoch   3 Batch   45/3125   train_loss = 0.809
2018-04-07T21:56:30.430820: Epoch   3 Batch   65/3125   train_loss = 0.975
2018-04-07T21:56:32.227640: Epoch   3 Batch   85/3125   train_loss = 0.839
2018-04-07T21:56:33.912117: Epoch   3 Batch  105/3125   train_loss = 0.766
2018-04-07T21:56:35.593627: Epoch   3 Batch  125/3125   train_loss = 0.938
2018-04-07T21:56:37.277781: Epoch   3 Batch  145/3125   train_loss = 0.918
2018-04-07T21:56:38.956243: Epoch   3 Batch  165/3125   train_loss = 0.966
2018-04-07T21:56:40.635287: Epoch   3 Batch  185/3125   train_loss = 0.827
2018-04-07T21:56:42.331678: Epoch   3 Batch  205/3125   train_loss = 0.779
2018-04-07T21:56:44.015153: Epoch   3 Batch  225/3125   train_loss = 0.827
2018-04-07T21:56:45.704141: Epoch   3 Batch  245/3125   train_loss = 1.119
2018-04-07T21:56:47.404196: Epoch   3 Batch  265/3125   train_loss = 0.859
2018-04-07T21:56:49.090851: Epoch   3 Batch  285/3125   train_loss = 0.953
2018-04-07T21:56:50.777334: Epoch   3 Batch  305/3125   train_loss = 0.840
2018-04-07T21:56:52.465944: Epoch   3 Batch  325/3125   train_loss = 0.921
2018-04-07T21:56:54.161415: Epoch   3 Batch  345/3125   train_loss = 0.946
2018-04-07T21:56:55.821936: Epoch   3 Batch  365/3125   train_loss = 0.872
2018-04-07T21:56:57.589726: Epoch   3 Batch  385/3125   train_loss = 0.919
2018-04-07T21:56:59.263796: Epoch   3 Batch  405/3125   train_loss = 0.873
2018-04-07T21:57:00.966240: Epoch   3 Batch  425/3125   train_loss = 0.942
2018-04-07T21:57:02.675797: Epoch   3 Batch  445/3125   train_loss = 0.936
2018-04-07T21:57:04.828032: Epoch   3 Batch  465/3125   train_loss = 0.905
2018-04-07T21:57:06.893521: Epoch   3 Batch  485/3125   train_loss = 1.060
2018-04-07T21:57:08.752462: Epoch   3 Batch  505/3125   train_loss = 0.862
2018-04-07T21:57:10.765814: Epoch   3 Batch  525/3125   train_loss = 0.992
2018-04-07T21:57:12.656878: Epoch   3 Batch  545/3125   train_loss = 0.833
2018-04-07T21:57:14.464724: Epoch   3 Batch  565/3125   train_loss = 1.111
2018-04-07T21:57:16.297093: Epoch   3 Batch  585/3125   train_loss = 0.874
2018-04-07T21:57:18.069466: Epoch   3 Batch  605/3125   train_loss = 0.895
2018-04-07T21:57:19.835160: Epoch   3 Batch  625/3125   train_loss = 0.864
2018-04-07T21:57:21.594877: Epoch   3 Batch  645/3125   train_loss = 0.942
2018-04-07T21:57:23.417730: Epoch   3 Batch  665/3125   train_loss = 1.034
2018-04-07T21:57:25.214105: Epoch   3 Batch  685/3125   train_loss = 0.933
2018-04-07T21:57:27.002359: Epoch   3 Batch  705/3125   train_loss = 1.094
2018-04-07T21:57:28.796628: Epoch   3 Batch  725/3125   train_loss = 0.947
2018-04-07T21:57:30.533257: Epoch   3 Batch  745/3125   train_loss = 0.863
2018-04-07T21:57:32.240059: Epoch   3 Batch  765/3125   train_loss = 0.896
2018-04-07T21:57:34.071026: Epoch   3 Batch  785/3125   train_loss = 1.047
2018-04-07T21:57:35.779174: Epoch   3 Batch  805/3125   train_loss = 0.872
2018-04-07T21:57:37.574947: Epoch   3 Batch  825/3125   train_loss = 0.937
2018-04-07T21:57:39.381248: Epoch   3 Batch  845/3125   train_loss = 0.896
2018-04-07T21:57:41.133411: Epoch   3 Batch  865/3125   train_loss = 0.980
2018-04-07T21:57:43.040479: Epoch   3 Batch  885/3125   train_loss = 0.940
2018-04-07T21:57:45.007730: Epoch   3 Batch  905/3125   train_loss = 0.981
2018-04-07T21:57:46.819045: Epoch   3 Batch  925/3125   train_loss = 0.932
2018-04-07T21:57:48.595796: Epoch   3 Batch  945/3125   train_loss = 0.970
2018-04-07T21:57:50.359484: Epoch   3 Batch  965/3125   train_loss = 0.744
2018-04-07T21:57:52.139216: Epoch   3 Batch  985/3125   train_loss = 0.935
2018-04-07T21:57:53.946425: Epoch   3 Batch 1005/3125   train_loss = 0.816
2018-04-07T21:57:55.723633: Epoch   3 Batch 1025/3125   train_loss = 0.889
2018-04-07T21:57:57.509416: Epoch   3 Batch 1045/3125   train_loss = 1.167
2018-04-07T21:57:59.397424: Epoch   3 Batch 1065/3125   train_loss = 0.952
2018-04-07T21:58:01.163631: Epoch   3 Batch 1085/3125   train_loss = 0.846
2018-04-07T21:58:02.976968: Epoch   3 Batch 1105/3125   train_loss = 0.863
2018-04-07T21:58:04.802320: Epoch   3 Batch 1125/3125   train_loss = 0.815
2018-04-07T21:58:06.595622: Epoch   3 Batch 1145/3125   train_loss = 0.885
2018-04-07T21:58:08.398399: Epoch   3 Batch 1165/3125   train_loss = 0.963
2018-04-07T21:58:10.181176: Epoch   3 Batch 1185/3125   train_loss = 0.852
2018-04-07T21:58:11.998547: Epoch   3 Batch 1205/3125   train_loss = 0.884
2018-04-07T21:58:13.755175: Epoch   3 Batch 1225/3125   train_loss = 0.923
2018-04-07T21:58:15.524929: Epoch   3 Batch 1245/3125   train_loss = 1.004
2018-04-07T21:58:17.318196: Epoch   3 Batch 1265/3125   train_loss = 0.871
2018-04-07T21:58:19.106554: Epoch   3 Batch 1285/3125   train_loss = 0.974
2018-04-07T21:58:20.815632: Epoch   3 Batch 1305/3125   train_loss = 0.793
2018-04-07T21:58:22.494613: Epoch   3 Batch 1325/3125   train_loss = 0.888
2018-04-07T21:58:24.176160: Epoch   3 Batch 1345/3125   train_loss = 0.913
2018-04-07T21:58:25.860142: Epoch   3 Batch 1365/3125   train_loss = 0.806
2018-04-07T21:58:27.553639: Epoch   3 Batch 1385/3125   train_loss = 0.794
2018-04-07T21:58:29.244248: Epoch   3 Batch 1405/3125   train_loss = 0.910
2018-04-07T21:58:30.947317: Epoch   3 Batch 1425/3125   train_loss = 1.016
2018-04-07T21:58:32.639922: Epoch   3 Batch 1445/3125   train_loss = 1.026
2018-04-07T21:58:34.331172: Epoch   3 Batch 1465/3125   train_loss = 0.859
2018-04-07T21:58:36.143705: Epoch   3 Batch 1485/3125   train_loss = 0.943
2018-04-07T21:58:37.820180: Epoch   3 Batch 1505/3125   train_loss = 0.757
2018-04-07T21:58:39.503334: Epoch   3 Batch 1525/3125   train_loss = 0.803
2018-04-07T21:58:41.199371: Epoch   3 Batch 1545/3125   train_loss = 0.908
2018-04-07T21:58:42.879399: Epoch   3 Batch 1565/3125   train_loss = 0.943
2018-04-07T21:58:44.579537: Epoch   3 Batch 1585/3125   train_loss = 0.830
2018-04-07T21:58:46.275858: Epoch   3 Batch 1605/3125   train_loss = 0.919
2018-04-07T21:58:47.973053: Epoch   3 Batch 1625/3125   train_loss = 0.982
2018-04-07T21:58:49.659135: Epoch   3 Batch 1645/3125   train_loss = 1.075
2018-04-07T21:58:51.353750: Epoch   3 Batch 1665/3125   train_loss = 0.946
2018-04-07T21:58:53.071066: Epoch   3 Batch 1685/3125   train_loss = 1.001
2018-04-07T21:58:54.749104: Epoch   3 Batch 1705/3125   train_loss = 0.918
2018-04-07T21:58:56.411015: Epoch   3 Batch 1725/3125   train_loss = 0.857
2018-04-07T21:58:58.090095: Epoch   3 Batch 1745/3125   train_loss = 0.800
2018-04-07T21:58:59.876193: Epoch   3 Batch 1765/3125   train_loss = 0.863
2018-04-07T21:59:01.556609: Epoch   3 Batch 1785/3125   train_loss = 1.028
2018-04-07T21:59:03.259417: Epoch   3 Batch 1805/3125   train_loss = 0.978
2018-04-07T21:59:04.955761: Epoch   3 Batch 1825/3125   train_loss = 1.037
2018-04-07T21:59:06.631960: Epoch   3 Batch 1845/3125   train_loss = 0.952
2018-04-07T21:59:08.332536: Epoch   3 Batch 1865/3125   train_loss = 0.807
2018-04-07T21:59:10.026042: Epoch   3 Batch 1885/3125   train_loss = 0.990
2018-04-07T21:59:11.720737: Epoch   3 Batch 1905/3125   train_loss = 0.837
2018-04-07T21:59:13.407442: Epoch   3 Batch 1925/3125   train_loss = 0.898
2018-04-07T21:59:15.095963: Epoch   3 Batch 1945/3125   train_loss = 0.913
2018-04-07T21:59:16.767229: Epoch   3 Batch 1965/3125   train_loss = 0.845
2018-04-07T21:59:18.428181: Epoch   3 Batch 1985/3125   train_loss = 0.915
2018-04-07T21:59:20.104114: Epoch   3 Batch 2005/3125   train_loss = 0.885
2018-04-07T21:59:21.788089: Epoch   3 Batch 2025/3125   train_loss = 0.906
2018-04-07T21:59:23.455601: Epoch   3 Batch 2045/3125   train_loss = 0.778
2018-04-07T21:59:25.143579: Epoch   3 Batch 2065/3125   train_loss = 0.746
2018-04-07T21:59:26.838188: Epoch   3 Batch 2085/3125   train_loss = 1.007
2018-04-07T21:59:28.523663: Epoch   3 Batch 2105/3125   train_loss = 0.858
2018-04-07T21:59:30.221180: Epoch   3 Batch 2125/3125   train_loss = 0.941
2018-04-07T21:59:31.898217: Epoch   3 Batch 2145/3125   train_loss = 0.945
2018-04-07T21:59:33.690121: Epoch   3 Batch 2165/3125   train_loss = 0.840
2018-04-07T21:59:35.383852: Epoch   3 Batch 2185/3125   train_loss = 0.946
2018-04-07T21:59:37.092413: Epoch   3 Batch 2205/3125   train_loss = 0.914
2018-04-07T21:59:38.767983: Epoch   3 Batch 2225/3125   train_loss = 0.910
2018-04-07T21:59:40.452680: Epoch   3 Batch 2245/3125   train_loss = 0.790
2018-04-07T21:59:42.138180: Epoch   3 Batch 2265/3125   train_loss = 0.986
2018-04-07T21:59:43.833252: Epoch   3 Batch 2285/3125   train_loss = 1.061
2018-04-07T21:59:45.511750: Epoch   3 Batch 2305/3125   train_loss = 0.876
2018-04-07T21:59:47.202750: Epoch   3 Batch 2325/3125   train_loss = 0.844
2018-04-07T21:59:48.882271: Epoch   3 Batch 2345/3125   train_loss = 0.860
2018-04-07T21:59:50.561695: Epoch   3 Batch 2365/3125   train_loss = 0.754
2018-04-07T21:59:52.267322: Epoch   3 Batch 2385/3125   train_loss = 0.941
2018-04-07T21:59:54.038564: Epoch   3 Batch 2405/3125   train_loss = 0.868
2018-04-07T21:59:55.710395: Epoch   3 Batch 2425/3125   train_loss = 0.829
2018-04-07T21:59:57.500734: Epoch   3 Batch 2445/3125   train_loss = 0.958
2018-04-07T21:59:59.189723: Epoch   3 Batch 2465/3125   train_loss = 0.740
2018-04-07T22:00:00.869725: Epoch   3 Batch 2485/3125   train_loss = 0.807
2018-04-07T22:00:02.564816: Epoch   3 Batch 2505/3125   train_loss = 0.855
2018-04-07T22:00:04.333399: Epoch   3 Batch 2525/3125   train_loss = 0.925
2018-04-07T22:00:06.019418: Epoch   3 Batch 2545/3125   train_loss = 0.994
2018-04-07T22:00:07.710485: Epoch   3 Batch 2565/3125   train_loss = 0.858
2018-04-07T22:00:09.398519: Epoch   3 Batch 2585/3125   train_loss = 0.791
2018-04-07T22:00:11.078917: Epoch   3 Batch 2605/3125   train_loss = 0.851
2018-04-07T22:00:12.773950: Epoch   3 Batch 2625/3125   train_loss = 0.999
2018-04-07T22:00:14.472536: Epoch   3 Batch 2645/3125   train_loss = 0.885
2018-04-07T22:00:16.156512: Epoch   3 Batch 2665/3125   train_loss = 0.962
2018-04-07T22:00:17.845501: Epoch   3 Batch 2685/3125   train_loss = 0.909
2018-04-07T22:00:19.542759: Epoch   3 Batch 2705/3125   train_loss = 0.788
2018-04-07T22:00:21.217682: Epoch   3 Batch 2725/3125   train_loss = 1.025
2018-04-07T22:00:22.895648: Epoch   3 Batch 2745/3125   train_loss = 0.956
2018-04-07T22:00:24.579824: Epoch   3 Batch 2765/3125   train_loss = 0.859
2018-04-07T22:00:26.241120: Epoch   3 Batch 2785/3125   train_loss = 1.022
2018-04-07T22:00:28.185607: Epoch   3 Batch 2805/3125   train_loss = 0.843
2018-04-07T22:00:29.870108: Epoch   3 Batch 2825/3125   train_loss = 0.890
2018-04-07T22:00:31.579398: Epoch   3 Batch 2845/3125   train_loss = 0.888
2018-04-07T22:00:33.351852: Epoch   3 Batch 2865/3125   train_loss = 0.868
2018-04-07T22:00:35.017964: Epoch   3 Batch 2885/3125   train_loss = 0.961
2018-04-07T22:00:36.715372: Epoch   3 Batch 2905/3125   train_loss = 0.966
2018-04-07T22:00:38.370713: Epoch   3 Batch 2925/3125   train_loss = 0.868
2018-04-07T22:00:40.223652: Epoch   3 Batch 2945/3125   train_loss = 0.941
2018-04-07T22:00:41.958290: Epoch   3 Batch 2965/3125   train_loss = 0.996
2018-04-07T22:00:43.630094: Epoch   3 Batch 2985/3125   train_loss = 0.832
2018-04-07T22:00:45.326743: Epoch   3 Batch 3005/3125   train_loss = 0.776
2018-04-07T22:00:46.996996: Epoch   3 Batch 3025/3125   train_loss = 0.950
2018-04-07T22:00:48.692106: Epoch   3 Batch 3045/3125   train_loss = 0.836
2018-04-07T22:00:50.385946: Epoch   3 Batch 3065/3125   train_loss = 0.809
2018-04-07T22:00:52.050411: Epoch   3 Batch 3085/3125   train_loss = 0.859
2018-04-07T22:00:53.696592: Epoch   3 Batch 3105/3125   train_loss = 0.915
2018-04-07T22:00:55.586518: Epoch   3 Batch   17/781   test_loss = 0.878
2018-04-07T22:00:55.889330: Epoch   3 Batch   37/781   test_loss = 0.888
2018-04-07T22:00:56.197415: Epoch   3 Batch   57/781   test_loss = 0.964
2018-04-07T22:00:56.507609: Epoch   3 Batch   77/781   test_loss = 0.855
2018-04-07T22:00:56.816192: Epoch   3 Batch   97/781   test_loss = 0.775
2018-04-07T22:00:57.123534: Epoch   3 Batch  117/781   test_loss = 0.925
2018-04-07T22:00:57.427955: Epoch   3 Batch  137/781   test_loss = 0.906
2018-04-07T22:00:57.739877: Epoch   3 Batch  157/781   test_loss = 0.968
2018-04-07T22:00:58.046215: Epoch   3 Batch  177/781   test_loss = 0.839
2018-04-07T22:00:58.354852: Epoch   3 Batch  197/781   test_loss = 0.953
2018-04-07T22:00:58.659380: Epoch   3 Batch  217/781   test_loss = 0.726
2018-04-07T22:00:58.967699: Epoch   3 Batch  237/781   test_loss = 0.819
2018-04-07T22:00:59.266476: Epoch   3 Batch  257/781   test_loss = 1.012
2018-04-07T22:00:59.568807: Epoch   3 Batch  277/781   test_loss = 0.944
2018-04-07T22:00:59.878745: Epoch   3 Batch  297/781   test_loss = 1.031
2018-04-07T22:01:00.181865: Epoch   3 Batch  317/781   test_loss = 1.010
2018-04-07T22:01:00.483200: Epoch   3 Batch  337/781   test_loss = 0.942
2018-04-07T22:01:00.793934: Epoch   3 Batch  357/781   test_loss = 0.886
2018-04-07T22:01:01.100340: Epoch   3 Batch  377/781   test_loss = 0.916
2018-04-07T22:01:01.410464: Epoch   3 Batch  397/781   test_loss = 0.981
2018-04-07T22:01:01.716989: Epoch   3 Batch  417/781   test_loss = 0.816
2018-04-07T22:01:02.021509: Epoch   3 Batch  437/781   test_loss = 0.834
2018-04-07T22:01:02.326559: Epoch   3 Batch  457/781   test_loss = 0.760
2018-04-07T22:01:02.635841: Epoch   3 Batch  477/781   test_loss = 0.897
2018-04-07T22:01:02.945128: Epoch   3 Batch  497/781   test_loss = 0.799
2018-04-07T22:01:03.247607: Epoch   3 Batch  517/781   test_loss = 0.844
2018-04-07T22:01:03.552683: Epoch   3 Batch  537/781   test_loss = 0.880
2018-04-07T22:01:03.852127: Epoch   3 Batch  557/781   test_loss = 1.018
2018-04-07T22:01:04.158654: Epoch   3 Batch  577/781   test_loss = 0.948
2018-04-07T22:01:04.469125: Epoch   3 Batch  597/781   test_loss = 0.851
2018-04-07T22:01:04.770647: Epoch   3 Batch  617/781   test_loss = 0.882
2018-04-07T22:01:05.078906: Epoch   3 Batch  637/781   test_loss = 0.814
2018-04-07T22:01:05.381146: Epoch   3 Batch  657/781   test_loss = 1.008
2018-04-07T22:01:05.693636: Epoch   3 Batch  677/781   test_loss = 0.944
2018-04-07T22:01:06.006299: Epoch   3 Batch  697/781   test_loss = 0.925
2018-04-07T22:01:06.311438: Epoch   3 Batch  717/781   test_loss = 0.847
2018-04-07T22:01:06.614949: Epoch   3 Batch  737/781   test_loss = 0.717
2018-04-07T22:01:06.919914: Epoch   3 Batch  757/781   test_loss = 1.019
2018-04-07T22:01:07.226839: Epoch   3 Batch  777/781   test_loss = 0.945
2018-04-07T22:01:07.962925: Epoch   4 Batch    0/3125   train_loss = 0.990
2018-04-07T22:01:09.726584: Epoch   4 Batch   20/3125   train_loss = 0.941
2018-04-07T22:01:11.420903: Epoch   4 Batch   40/3125   train_loss = 0.935
2018-04-07T22:01:13.093099: Epoch   4 Batch   60/3125   train_loss = 0.757
2018-04-07T22:01:14.738569: Epoch   4 Batch   80/3125   train_loss = 0.834
2018-04-07T22:01:16.369934: Epoch   4 Batch  100/3125   train_loss = 0.933
2018-04-07T22:01:18.009485: Epoch   4 Batch  120/3125   train_loss = 0.960
2018-04-07T22:01:19.646024: Epoch   4 Batch  140/3125   train_loss = 0.897
2018-04-07T22:01:21.282389: Epoch   4 Batch  160/3125   train_loss = 0.757
2018-04-07T22:01:22.924500: Epoch   4 Batch  180/3125   train_loss = 0.909
2018-04-07T22:01:24.572039: Epoch   4 Batch  200/3125   train_loss = 1.036
2018-04-07T22:01:26.215556: Epoch   4 Batch  220/3125   train_loss = 0.873
2018-04-07T22:01:27.876982: Epoch   4 Batch  240/3125   train_loss = 0.986
2018-04-07T22:01:29.505617: Epoch   4 Batch  260/3125   train_loss = 0.889
2018-04-07T22:01:31.146570: Epoch   4 Batch  280/3125   train_loss = 0.954
2018-04-07T22:01:32.787767: Epoch   4 Batch  300/3125   train_loss = 1.081
2018-04-07T22:01:34.428352: Epoch   4 Batch  320/3125   train_loss = 0.927
2018-04-07T22:01:36.071681: Epoch   4 Batch  340/3125   train_loss = 0.756
2018-04-07T22:01:37.717952: Epoch   4 Batch  360/3125   train_loss = 0.812
2018-04-07T22:01:39.376390: Epoch   4 Batch  380/3125   train_loss = 0.863
2018-04-07T22:01:41.016830: Epoch   4 Batch  400/3125   train_loss = 0.811
2018-04-07T22:01:42.763328: Epoch   4 Batch  420/3125   train_loss = 0.780
2018-04-07T22:01:44.414363: Epoch   4 Batch  440/3125   train_loss = 0.873
2018-04-07T22:01:46.052782: Epoch   4 Batch  460/3125   train_loss = 0.848
2018-04-07T22:01:47.702662: Epoch   4 Batch  480/3125   train_loss = 1.031
2018-04-07T22:01:49.345352: Epoch   4 Batch  500/3125   train_loss = 0.674
2018-04-07T22:01:50.987652: Epoch   4 Batch  520/3125   train_loss = 0.945
2018-04-07T22:01:52.629957: Epoch   4 Batch  540/3125   train_loss = 0.843
2018-04-07T22:01:54.270296: Epoch   4 Batch  560/3125   train_loss = 1.018
2018-04-07T22:01:55.923154: Epoch   4 Batch  580/3125   train_loss = 0.974
2018-04-07T22:01:57.579210: Epoch   4 Batch  600/3125   train_loss = 0.919
2018-04-07T22:01:59.223764: Epoch   4 Batch  620/3125   train_loss = 0.924
2018-04-07T22:02:00.877474: Epoch   4 Batch  640/3125   train_loss = 0.843
2018-04-07T22:02:02.511168: Epoch   4 Batch  660/3125   train_loss = 0.874
2018-04-07T22:02:04.298291: Epoch   4 Batch  680/3125   train_loss = 0.965
2018-04-07T22:02:06.047962: Epoch   4 Batch  700/3125   train_loss = 0.918
2018-04-07T22:02:07.688246: Epoch   4 Batch  720/3125   train_loss = 0.814
2018-04-07T22:02:09.330694: Epoch   4 Batch  740/3125   train_loss = 0.937
2018-04-07T22:02:10.986966: Epoch   4 Batch  760/3125   train_loss = 0.826
2018-04-07T22:02:12.627308: Epoch   4 Batch  780/3125   train_loss = 0.933
2018-04-07T22:02:14.265058: Epoch   4 Batch  800/3125   train_loss = 0.784
2018-04-07T22:02:15.901364: Epoch   4 Batch  820/3125   train_loss = 0.857
2018-04-07T22:02:17.545067: Epoch   4 Batch  840/3125   train_loss = 0.879
2018-04-07T22:02:19.210844: Epoch   4 Batch  860/3125   train_loss = 0.838
2018-04-07T22:02:20.852719: Epoch   4 Batch  880/3125   train_loss = 0.759
2018-04-07T22:02:22.490446: Epoch   4 Batch  900/3125   train_loss = 0.851
2018-04-07T22:02:24.144358: Epoch   4 Batch  920/3125   train_loss = 0.904
2018-04-07T22:02:25.787528: Epoch   4 Batch  940/3125   train_loss = 0.885
2018-04-07T22:02:27.441933: Epoch   4 Batch  960/3125   train_loss = 0.921
2018-04-07T22:02:29.093742: Epoch   4 Batch  980/3125   train_loss = 0.998
2018-04-07T22:02:30.742197: Epoch   4 Batch 1000/3125   train_loss = 0.938
2018-04-07T22:02:32.388670: Epoch   4 Batch 1020/3125   train_loss = 0.913
2018-04-07T22:02:34.055392: Epoch   4 Batch 1040/3125   train_loss = 0.776
2018-04-07T22:02:35.705811: Epoch   4 Batch 1060/3125   train_loss = 0.958
2018-04-07T22:02:37.350128: Epoch   4 Batch 1080/3125   train_loss = 0.959
2018-04-07T22:02:39.026306: Epoch   4 Batch 1100/3125   train_loss = 0.870
2018-04-07T22:02:40.837145: Epoch   4 Batch 1120/3125   train_loss = 0.883
2018-04-07T22:02:42.473220: Epoch   4 Batch 1140/3125   train_loss = 0.873
2018-04-07T22:02:44.116996: Epoch   4 Batch 1160/3125   train_loss = 0.841
2018-04-07T22:02:45.759997: Epoch   4 Batch 1180/3125   train_loss = 0.799
2018-04-07T22:02:47.409495: Epoch   4 Batch 1200/3125   train_loss = 1.005
2018-04-07T22:02:49.061464: Epoch   4 Batch 1220/3125   train_loss = 0.928
2018-04-07T22:02:50.714821: Epoch   4 Batch 1240/3125   train_loss = 0.803
2018-04-07T22:02:52.373305: Epoch   4 Batch 1260/3125   train_loss = 0.908
2018-04-07T22:02:54.015587: Epoch   4 Batch 1280/3125   train_loss = 0.914
2018-04-07T22:02:55.667068: Epoch   4 Batch 1300/3125   train_loss = 0.814
2018-04-07T22:02:57.315347: Epoch   4 Batch 1320/3125   train_loss = 0.909
2018-04-07T22:02:58.967244: Epoch   4 Batch 1340/3125   train_loss = 0.764
2018-04-07T22:03:00.617373: Epoch   4 Batch 1360/3125   train_loss = 0.864
2018-04-07T22:03:02.281183: Epoch   4 Batch 1380/3125   train_loss = 0.780
2018-04-07T22:03:04.033899: Epoch   4 Batch 1400/3125   train_loss = 0.949
2018-04-07T22:03:05.677891: Epoch   4 Batch 1420/3125   train_loss = 0.871
2018-04-07T22:03:07.328174: Epoch   4 Batch 1440/3125   train_loss = 0.791
2018-04-07T22:03:08.984655: Epoch   4 Batch 1460/3125   train_loss = 0.929
2018-04-07T22:03:10.636162: Epoch   4 Batch 1480/3125   train_loss = 0.916
2018-04-07T22:03:12.284231: Epoch   4 Batch 1500/3125   train_loss = 0.892
2018-04-07T22:03:13.936441: Epoch   4 Batch 1520/3125   train_loss = 0.819
2018-04-07T22:03:15.572303: Epoch   4 Batch 1540/3125   train_loss = 0.986
2018-04-07T22:03:17.221455: Epoch   4 Batch 1560/3125   train_loss = 0.800
2018-04-07T22:03:18.874698: Epoch   4 Batch 1580/3125   train_loss = 1.005
2018-04-07T22:03:20.521852: Epoch   4 Batch 1600/3125   train_loss = 0.812
2018-04-07T22:03:22.165419: Epoch   4 Batch 1620/3125   train_loss = 0.811
2018-04-07T22:03:23.796882: Epoch   4 Batch 1640/3125   train_loss = 0.968
2018-04-07T22:03:25.447290: Epoch   4 Batch 1660/3125   train_loss = 0.984
2018-04-07T22:03:27.091027: Epoch   4 Batch 1680/3125   train_loss = 0.813
2018-04-07T22:03:28.740529: Epoch   4 Batch 1700/3125   train_loss = 0.813
2018-04-07T22:03:30.389789: Epoch   4 Batch 1720/3125   train_loss = 0.848
2018-04-07T22:03:32.051858: Epoch   4 Batch 1740/3125   train_loss = 0.924
2018-04-07T22:03:33.697736: Epoch   4 Batch 1760/3125   train_loss = 0.870
2018-04-07T22:03:35.353375: Epoch   4 Batch 1780/3125   train_loss = 0.860
2018-04-07T22:03:37.105225: Epoch   4 Batch 1800/3125   train_loss = 0.808
2018-04-07T22:03:38.756880: Epoch   4 Batch 1820/3125   train_loss = 0.820
2018-04-07T22:03:40.411859: Epoch   4 Batch 1840/3125   train_loss = 0.944
2018-04-07T22:03:42.067488: Epoch   4 Batch 1860/3125   train_loss = 0.947
2018-04-07T22:03:43.747523: Epoch   4 Batch 1880/3125   train_loss = 0.934
2018-04-07T22:03:45.389904: Epoch   4 Batch 1900/3125   train_loss = 0.758
2018-04-07T22:03:47.041420: Epoch   4 Batch 1920/3125   train_loss = 0.890
2018-04-07T22:03:48.704373: Epoch   4 Batch 1940/3125   train_loss = 0.733
2018-04-07T22:03:50.361970: Epoch   4 Batch 1960/3125   train_loss = 0.754
2018-04-07T22:03:52.018180: Epoch   4 Batch 1980/3125   train_loss = 0.884
2018-04-07T22:03:53.668957: Epoch   4 Batch 2000/3125   train_loss = 1.049
2018-04-07T22:03:55.317327: Epoch   4 Batch 2020/3125   train_loss = 0.994
2018-04-07T22:03:56.961459: Epoch   4 Batch 2040/3125   train_loss = 0.828
2018-04-07T22:03:58.608284: Epoch   4 Batch 2060/3125   train_loss = 0.803
2018-04-07T22:04:00.374238: Epoch   4 Batch 2080/3125   train_loss = 1.015
2018-04-07T22:04:02.028204: Epoch   4 Batch 2100/3125   train_loss = 0.832
2018-04-07T22:04:03.699140: Epoch   4 Batch 2120/3125   train_loss = 0.792
2018-04-07T22:04:05.410170: Epoch   4 Batch 2140/3125   train_loss = 0.850
2018-04-07T22:04:07.064845: Epoch   4 Batch 2160/3125   train_loss = 0.863
2018-04-07T22:04:08.723793: Epoch   4 Batch 2180/3125   train_loss = 0.898
2018-04-07T22:04:10.370374: Epoch   4 Batch 2200/3125   train_loss = 0.825
2018-04-07T22:04:12.030095: Epoch   4 Batch 2220/3125   train_loss = 0.868
2018-04-07T22:04:13.676153: Epoch   4 Batch 2240/3125   train_loss = 0.851
2018-04-07T22:04:15.324987: Epoch   4 Batch 2260/3125   train_loss = 0.879
2018-04-07T22:04:16.974515: Epoch   4 Batch 2280/3125   train_loss = 0.897
2018-04-07T22:04:18.615541: Epoch   4 Batch 2300/3125   train_loss = 0.923
2018-04-07T22:04:20.269808: Epoch   4 Batch 2320/3125   train_loss = 0.986
2018-04-07T22:04:21.952723: Epoch   4 Batch 2340/3125   train_loss = 0.906
2018-04-07T22:04:23.692100: Epoch   4 Batch 2360/3125   train_loss = 0.945
2018-04-07T22:04:25.341581: Epoch   4 Batch 2380/3125   train_loss = 0.844
2018-04-07T22:04:26.997790: Epoch   4 Batch 2400/3125   train_loss = 0.987
2018-04-07T22:04:28.654865: Epoch   4 Batch 2420/3125   train_loss = 0.779
2018-04-07T22:04:30.298304: Epoch   4 Batch 2440/3125   train_loss = 0.817
2018-04-07T22:04:31.950959: Epoch   4 Batch 2460/3125   train_loss = 0.890
2018-04-07T22:04:33.601223: Epoch   4 Batch 2480/3125   train_loss = 0.965
2018-04-07T22:04:35.368894: Epoch   4 Batch 2500/3125   train_loss = 0.814
2018-04-07T22:04:37.019489: Epoch   4 Batch 2520/3125   train_loss = 0.928
2018-04-07T22:04:38.681238: Epoch   4 Batch 2540/3125   train_loss = 0.835
2018-04-07T22:04:40.362265: Epoch   4 Batch 2560/3125   train_loss = 0.624
2018-04-07T22:04:42.000541: Epoch   4 Batch 2580/3125   train_loss = 0.822
2018-04-07T22:04:43.645570: Epoch   4 Batch 2600/3125   train_loss = 0.878
2018-04-07T22:04:45.287514: Epoch   4 Batch 2620/3125   train_loss = 0.758
2018-04-07T22:04:46.933734: Epoch   4 Batch 2640/3125   train_loss = 0.825
2018-04-07T22:04:48.590285: Epoch   4 Batch 2660/3125   train_loss = 0.998
2018-04-07T22:04:50.258512: Epoch   4 Batch 2680/3125   train_loss = 0.796
2018-04-07T22:04:51.899020: Epoch   4 Batch 2700/3125   train_loss = 0.863
2018-04-07T22:04:53.559619: Epoch   4 Batch 2720/3125   train_loss = 0.767
2018-04-07T22:04:55.225229: Epoch   4 Batch 2740/3125   train_loss = 0.856
2018-04-07T22:04:56.865569: Epoch   4 Batch 2760/3125   train_loss = 0.776
2018-04-07T22:04:58.627230: Epoch   4 Batch 2780/3125   train_loss = 0.832
2018-04-07T22:05:00.281611: Epoch   4 Batch 2800/3125   train_loss = 1.034
2018-04-07T22:05:01.937232: Epoch   4 Batch 2820/3125   train_loss = 1.031
2018-04-07T22:05:03.589732: Epoch   4 Batch 2840/3125   train_loss = 0.823
2018-04-07T22:05:05.252147: Epoch   4 Batch 2860/3125   train_loss = 0.779
2018-04-07T22:05:06.912645: Epoch   4 Batch 2880/3125   train_loss = 0.835
2018-04-07T22:05:08.561089: Epoch   4 Batch 2900/3125   train_loss = 0.833
2018-04-07T22:05:10.226568: Epoch   4 Batch 2920/3125   train_loss = 0.854
2018-04-07T22:05:11.883458: Epoch   4 Batch 2940/3125   train_loss = 0.952
2018-04-07T22:05:13.542393: Epoch   4 Batch 2960/3125   train_loss = 0.905
2018-04-07T22:05:15.208086: Epoch   4 Batch 2980/3125   train_loss = 0.879
2018-04-07T22:05:16.861136: Epoch   4 Batch 3000/3125   train_loss = 0.969
2018-04-07T22:05:18.498268: Epoch   4 Batch 3020/3125   train_loss = 0.994
2018-04-07T22:05:20.142812: Epoch   4 Batch 3040/3125   train_loss = 0.927
2018-04-07T22:05:21.788241: Epoch   4 Batch 3060/3125   train_loss = 0.764
2018-04-07T22:05:23.434645: Epoch   4 Batch 3080/3125   train_loss = 0.977
2018-04-07T22:05:25.082388: Epoch   4 Batch 3100/3125   train_loss = 1.046
2018-04-07T22:05:26.737321: Epoch   4 Batch 3120/3125   train_loss = 0.852
2018-04-07T22:05:27.396219: Epoch   4 Batch   16/781   test_loss = 0.843
2018-04-07T22:05:27.707096: Epoch   4 Batch   36/781   test_loss = 0.925
2018-04-07T22:05:28.008618: Epoch   4 Batch   56/781   test_loss = 0.965
2018-04-07T22:05:28.315059: Epoch   4 Batch   76/781   test_loss = 0.995
2018-04-07T22:05:28.625637: Epoch   4 Batch   96/781   test_loss = 0.965
2018-04-07T22:05:28.932274: Epoch   4 Batch  116/781   test_loss = 0.858
2018-04-07T22:05:29.236907: Epoch   4 Batch  136/781   test_loss = 0.829
2018-04-07T22:05:29.544206: Epoch   4 Batch  156/781   test_loss = 0.932
2018-04-07T22:05:29.853598: Epoch   4 Batch  176/781   test_loss = 0.873
2018-04-07T22:05:30.159955: Epoch   4 Batch  196/781   test_loss = 0.823
2018-04-07T22:05:30.464096: Epoch   4 Batch  216/781   test_loss = 0.962
2018-04-07T22:05:30.773826: Epoch   4 Batch  236/781   test_loss = 0.814
2018-04-07T22:05:31.082942: Epoch   4 Batch  256/781   test_loss = 0.858
2018-04-07T22:05:31.385382: Epoch   4 Batch  276/781   test_loss = 1.075
2018-04-07T22:05:31.693032: Epoch   4 Batch  296/781   test_loss = 0.854
2018-04-07T22:05:31.996680: Epoch   4 Batch  316/781   test_loss = 0.830
2018-04-07T22:05:32.298411: Epoch   4 Batch  336/781   test_loss = 0.787
2018-04-07T22:05:32.601853: Epoch   4 Batch  356/781   test_loss = 0.827
2018-04-07T22:05:32.910596: Epoch   4 Batch  376/781   test_loss = 0.905
2018-04-07T22:05:33.217772: Epoch   4 Batch  396/781   test_loss = 0.845
2018-04-07T22:05:33.522075: Epoch   4 Batch  416/781   test_loss = 0.954
2018-04-07T22:05:33.827062: Epoch   4 Batch  436/781   test_loss = 0.974
2018-04-07T22:05:34.131500: Epoch   4 Batch  456/781   test_loss = 0.742
2018-04-07T22:05:34.431739: Epoch   4 Batch  476/781   test_loss = 0.969
2018-04-07T22:05:34.742070: Epoch   4 Batch  496/781   test_loss = 0.979
2018-04-07T22:05:35.047322: Epoch   4 Batch  516/781   test_loss = 0.782
2018-04-07T22:05:35.350685: Epoch   4 Batch  536/781   test_loss = 0.983
2018-04-07T22:05:35.658226: Epoch   4 Batch  556/781   test_loss = 0.879
2018-04-07T22:05:35.959265: Epoch   4 Batch  576/781   test_loss = 0.949
2018-04-07T22:05:36.266328: Epoch   4 Batch  596/781   test_loss = 0.963
2018-04-07T22:05:36.566093: Epoch   4 Batch  616/781   test_loss = 0.922
2018-04-07T22:05:36.870105: Epoch   4 Batch  636/781   test_loss = 0.832
2018-04-07T22:05:37.176260: Epoch   4 Batch  656/781   test_loss = 0.867
2018-04-07T22:05:37.485733: Epoch   4 Batch  676/781   test_loss = 1.074
2018-04-07T22:05:37.798345: Epoch   4 Batch  696/781   test_loss = 0.836
2018-04-07T22:05:38.098603: Epoch   4 Batch  716/781   test_loss = 0.870
2018-04-07T22:05:38.398719: Epoch   4 Batch  736/781   test_loss = 1.018
2018-04-07T22:05:38.701120: Epoch   4 Batch  756/781   test_loss = 0.846
2018-04-07T22:05:39.001705: Epoch   4 Batch  776/781   test_loss = 0.786
Model Trained and Saved
2018-04-07 22:05:44.609054: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0)
2018-04-07 22:05:52.304461: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0)
2018-04-07 22:06:14.319556: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0)
2018-04-07 22:06:48.866588: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0)
[1401 'Ghosts of Mississippi (1996)' 'Drama']

61
[62 "Mr. Holland's Opus (1995)" 'Drama']
747
[757 'Ashes of Time (1994)' 'Drama']
2364
[2433 'Civil Action, A (1998)' 'Drama']
3117
[3186 'Girl, Interrupted (1999)' 'Drama']
165
[167 'Feast of July (1995)' 'Drama']
2018-04-07 22:06:57.086815: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0)

113
[115 'Happiness Is in the Field (1995)' 'Comedy']
802
[812 'Magic Hunter (1994)' 'Drama']
892
[904 'Rear Window (1954)' 'Mystery|Thriller']
740
[750
 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)'
 'Sci-Fi|War']
1242
[1262 'Great Escape, The (1963)' 'Adventure|War']
2018-04-07 22:07:05.195641: I c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\common_runtime\gpu\gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 950M, pci bus id: 0000:01:00.0)
[1401 'Ghosts of Mississippi (1996)' 'Drama']
[[5678 'M' 35 17]
 [881 'M' 18 14]
 [1644 'M' 18 12]
 [3935 'M' 35 18]
 [1170 'M' 56 3]
 [4200 'M' 45 7]
 [5493 'M' 35 12]
 [3833 'M' 25 1]
 [5448 'M' 45 19]
 [3591 'M' 35 6]
 [2405 'M' 50 8]
 [1763 'M' 35 7]
 [5755 'F' 35 2]
 [4814 'M' 18 14]
 [3901 'M' 18 14]
 [2154 'M' 25 12]
 [1855 'M' 18 4]
 [100 'M' 35 17]
 [4754 'F' 18 0]
 [2646 'M' 45 17]]

